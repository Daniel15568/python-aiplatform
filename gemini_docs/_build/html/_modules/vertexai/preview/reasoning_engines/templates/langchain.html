

<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>vertexai.preview.reasoning_engines.templates.langchain &#8212; google-cloud-aiplatform  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/alabaster.css" />
    <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../_static/jquery.js"></script>
    <script src="../../../../../_static/underscore.js"></script>
    <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>

  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
          	<div class="admonition" id="python2-eol"> 
          	 As of January 1, 2020 this library no longer supports Python 2 on the latest released version. 
          	 Library versions released prior to that date will continue to be available. For more information please
          	 visit <a href="https://cloud.google.com/python/docs/python2-sunset/">Python 2 support on Google Cloud</a>.
          	</div>
            
  <h1>Source code for vertexai.preview.reasoning_engines.templates.langchain</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1"># Copyright 2024 Google LLC</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">langchain_core</span> <span class="kn">import</span> <span class="n">runnables</span>
        <span class="kn">from</span> <span class="nn">langchain_core</span> <span class="kn">import</span> <span class="n">tools</span> <span class="k">as</span> <span class="n">lc_tools</span>
        <span class="kn">from</span> <span class="nn">langchain_core.language_models</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">lc_language_models</span>

        <span class="n">BaseTool</span> <span class="o">=</span> <span class="n">lc_tools</span><span class="o">.</span><span class="n">BaseTool</span>
        <span class="n">BaseLanguageModel</span> <span class="o">=</span> <span class="n">lc_language_models</span><span class="o">.</span><span class="n">BaseLanguageModel</span>
        <span class="n">GetSessionHistoryCallable</span> <span class="o">=</span> <span class="n">runnables</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">GetSessionHistoryCallable</span>
        <span class="n">RunnableConfig</span> <span class="o">=</span> <span class="n">runnables</span><span class="o">.</span><span class="n">RunnableConfig</span>
        <span class="n">RunnableSerializable</span> <span class="o">=</span> <span class="n">runnables</span><span class="o">.</span><span class="n">RunnableSerializable</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="n">BaseTool</span> <span class="o">=</span> <span class="n">Any</span>
        <span class="n">BaseLanguageModel</span> <span class="o">=</span> <span class="n">Any</span>
        <span class="n">GetSessionHistoryCallable</span> <span class="o">=</span> <span class="n">Any</span>
        <span class="n">RunnableConfig</span> <span class="o">=</span> <span class="n">Any</span>
        <span class="n">RunnableSerializable</span> <span class="o">=</span> <span class="n">Any</span>


<span class="k">def</span> <span class="nf">_default_runnable_kwargs</span><span class="p">(</span><span class="n">has_history</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="c1"># https://github.com/langchain-ai/langchain/blob/5784dfed001730530637793bea1795d9d5a7c244/libs/core/langchain_core/runnables/history.py#L237-L241</span>
    <span class="n">runnable_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># input_messages_key (str): Must be specified if the underlying</span>
        <span class="c1"># agent accepts a dict as input.</span>
        <span class="s2">&quot;input_messages_key&quot;</span><span class="p">:</span> <span class="s2">&quot;input&quot;</span><span class="p">,</span>
        <span class="c1"># output_messages_key (str): Must be specified if the underlying</span>
        <span class="c1"># agent returns a dict as output.</span>
        <span class="s2">&quot;output_messages_key&quot;</span><span class="p">:</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">has_history</span><span class="p">:</span>
        <span class="c1"># history_messages_key (str): Must be specified if the underlying</span>
        <span class="c1"># agent accepts a dict as input and a separate key for historical</span>
        <span class="c1"># messages.</span>
        <span class="n">runnable_kwargs</span><span class="p">[</span><span class="s2">&quot;history_messages_key&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;history&quot;</span>
    <span class="k">return</span> <span class="n">runnable_kwargs</span>


<span class="k">def</span> <span class="nf">_default_output_parser</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">langchain.agents.output_parsers.tools</span> <span class="kn">import</span> <span class="n">ToolsAgentOutputParser</span>
    <span class="k">return</span> <span class="n">ToolsAgentOutputParser</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_default_model_builder</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">project</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;BaseLanguageModel&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">vertexai</span>
    <span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">initializer</span>
    <span class="kn">from</span> <span class="nn">langchain_google_vertexai</span> <span class="kn">import</span> <span class="n">ChatVertexAI</span>

    <span class="n">model_kwargs</span> <span class="o">=</span> <span class="n">model_kwargs</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="n">current_project</span> <span class="o">=</span> <span class="n">initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">project</span>
    <span class="n">current_location</span> <span class="o">=</span> <span class="n">initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">location</span>
    <span class="n">vertexai</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">project</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">location</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ChatVertexAI</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
    <span class="n">vertexai</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">current_project</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="n">current_location</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">_default_runnable_builder</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="s2">&quot;BaseLanguageModel&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="s2">&quot;BaseTool&quot;</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;RunnableSerializable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_parser</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;RunnableSerializable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">chat_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;GetSessionHistoryCallable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">agent_executor_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">runnable_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;RunnableSerializable&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">langchain_core</span> <span class="kn">import</span> <span class="n">tools</span> <span class="k">as</span> <span class="n">lc_tools</span>
    <span class="kn">from</span> <span class="nn">langchain.agents</span> <span class="kn">import</span> <span class="n">AgentExecutor</span>
    <span class="kn">from</span> <span class="nn">langchain.tools.base</span> <span class="kn">import</span> <span class="n">StructuredTool</span>
    <span class="c1"># The prompt template and runnable_kwargs needs to be customized depending</span>
    <span class="c1"># on whether the user intends for the agent to have history. The way the</span>
    <span class="c1"># user would reflect that is by setting chat_history (which defaults to</span>
    <span class="c1"># None).</span>
    <span class="n">has_history</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">chat_history</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span> <span class="ow">or</span> <span class="n">_default_prompt</span><span class="p">(</span><span class="n">has_history</span><span class="p">)</span>
    <span class="n">output_parser</span> <span class="o">=</span> <span class="n">output_parser</span> <span class="ow">or</span> <span class="n">_default_output_parser</span><span class="p">()</span>
    <span class="n">agent_executor_kwargs</span> <span class="o">=</span> <span class="n">agent_executor_kwargs</span> <span class="ow">or</span> <span class="p">{}</span>
    <span class="n">runnable_kwargs</span> <span class="o">=</span> <span class="n">runnable_kwargs</span> <span class="ow">or</span> <span class="n">_default_runnable_kwargs</span><span class="p">(</span><span class="n">has_history</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tools</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tools</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">agent_executor</span> <span class="o">=</span> <span class="n">AgentExecutor</span><span class="p">(</span>
        <span class="n">agent</span><span class="o">=</span><span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="n">output_parser</span><span class="p">,</span>
        <span class="n">tools</span><span class="o">=</span><span class="p">[</span>
            <span class="n">tool</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool</span><span class="p">,</span> <span class="n">lc_tools</span><span class="o">.</span><span class="n">BaseTool</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">StructuredTool</span><span class="o">.</span><span class="n">from_function</span><span class="p">(</span><span class="n">tool</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span>
        <span class="p">],</span>
        <span class="o">**</span><span class="n">agent_executor_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">has_history</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">langchain_core.runnables.history</span> <span class="kn">import</span> <span class="n">RunnableWithMessageHistory</span>
        <span class="k">return</span> <span class="n">RunnableWithMessageHistory</span><span class="p">(</span>
            <span class="n">runnable</span><span class="o">=</span><span class="n">agent_executor</span><span class="p">,</span>
            <span class="n">get_session_history</span><span class="o">=</span><span class="n">chat_history</span><span class="p">,</span>
            <span class="o">**</span><span class="n">runnable_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">agent_executor</span>


<span class="k">def</span> <span class="nf">_default_prompt</span><span class="p">(</span><span class="n">has_history</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;RunnableSerializable&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">langchain_core</span> <span class="kn">import</span> <span class="n">prompts</span>
    <span class="kn">from</span> <span class="nn">langchain.agents.format_scratchpad.tools</span> <span class="kn">import</span> <span class="n">format_to_tool_messages</span>
    <span class="k">if</span> <span class="n">has_history</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;history&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;history&quot;</span><span class="p">],</span>
            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>
            <span class="s2">&quot;agent_scratchpad&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">format_to_tool_messages</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;intermediate_steps&quot;</span><span class="p">])</span>
            <span class="p">),</span>
        <span class="p">}</span> <span class="o">|</span> <span class="n">prompts</span><span class="o">.</span><span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
            <span class="n">prompts</span><span class="o">.</span><span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{input}</span><span class="s2">&quot;</span><span class="p">),</span>
            <span class="n">prompts</span><span class="o">.</span><span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;agent_scratchpad&quot;</span><span class="p">),</span>
        <span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>
            <span class="s2">&quot;agent_scratchpad&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">format_to_tool_messages</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;intermediate_steps&quot;</span><span class="p">])</span>
            <span class="p">),</span>
        <span class="p">}</span> <span class="o">|</span> <span class="n">prompts</span><span class="o">.</span><span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
            <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{input}</span><span class="s2">&quot;</span><span class="p">),</span>
            <span class="n">prompts</span><span class="o">.</span><span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;agent_scratchpad&quot;</span><span class="p">),</span>
        <span class="p">])</span>


<span class="k">def</span> <span class="nf">_validate_callable_parameters_are_annotated</span><span class="p">(</span><span class="nb">callable</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Validates that the parameters of the callable have type annotations.</span>

<span class="sd">    This ensures that they can be used for constructing LangChain tools that are</span>
<span class="sd">    usable with Gemini function calling.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">inspect</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="nb">callable</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">parameter</span><span class="o">.</span><span class="n">annotation</span> <span class="o">==</span> <span class="n">inspect</span><span class="o">.</span><span class="n">Parameter</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Callable=</span><span class="si">{</span><span class="nb">callable</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> has untyped input_arg=</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Please specify a type when defining it, e.g. `</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: str`.&quot;</span>
            <span class="p">)</span>


<span class="k">def</span> <span class="nf">_validate_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="s2">&quot;BaseTool&quot;</span><span class="p">]]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Validates that the tools are usable for tool calling.&quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">langchain_core</span> <span class="kn">import</span> <span class="n">tools</span> <span class="k">as</span> <span class="n">lc_tools</span>
    <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tool</span><span class="p">,</span> <span class="n">lc_tools</span><span class="o">.</span><span class="n">BaseTool</span><span class="p">):</span>
            <span class="n">_validate_callable_parameters_are_annotated</span><span class="p">(</span><span class="n">tool</span><span class="p">)</span>


<div class="viewcode-block" id="LangchainAgent"><a class="viewcode-back" href="../../../../../vertexai/services.html#vertexai.preview.reasoning_engines.LangchainAgent">[docs]</a><span class="k">class</span> <span class="nc">LangchainAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A Langchain Agent.</span>

<span class="sd">    Reference:</span>
<span class="sd">    *   Agent: https://python.langchain.com/docs/modules/agents/concepts</span>
<span class="sd">    *   Memory: https://python.langchain.com/docs/expression_language/how_to/message_history</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;RunnableSerializable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="s2">&quot;BaseTool&quot;</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_parser</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;RunnableSerializable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">chat_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;GetSessionHistoryCallable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">agent_executor_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runnable_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_builder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">runnable_builder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes the LangchainAgent.</span>

<span class="sd">        Under-the-hood, assuming .set_up() is called, this will correspond to</span>

<span class="sd">        ```</span>
<span class="sd">        model = model_builder(model_name=model, model_kwargs=model_kwargs)</span>
<span class="sd">        runnable = runnable_builder(</span>
<span class="sd">            prompt=prompt,</span>
<span class="sd">            model=model,</span>
<span class="sd">            tools=tools,</span>
<span class="sd">            output_parser=output_parser,</span>
<span class="sd">            chat_history=chat_history,</span>
<span class="sd">            agent_executor_kwargs=agent_executor_kwargs,</span>
<span class="sd">            runnable_kwargs=runnable_kwargs,</span>
<span class="sd">        )</span>
<span class="sd">        ```</span>

<span class="sd">        When everything is based on their default values, this corresponds to</span>
<span class="sd">        ```</span>
<span class="sd">        # model_builder</span>
<span class="sd">        from langchain_google_vertexai import ChatVertexAI</span>
<span class="sd">        llm = ChatVertexAI(model_name=model, **model_kwargs)</span>

<span class="sd">        # runnable_builder</span>
<span class="sd">        from langchain import agents</span>
<span class="sd">        from langchain_core.runnables.history import RunnableWithMessageHistory</span>
<span class="sd">        agent_executor = agents.AgentExecutor(</span>
<span class="sd">            agent=prompt | llm.bind_tools(tools=tools) | output_parser,</span>
<span class="sd">            tools=tools,</span>
<span class="sd">            **agent_executor_kwargs,</span>
<span class="sd">        )</span>
<span class="sd">        runnable = RunnableWithMessageHistory(</span>
<span class="sd">            runnable=agent_executor,</span>
<span class="sd">            get_session_history=chat_history,</span>
<span class="sd">            **runnable_kwargs,</span>
<span class="sd">        )</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            model (str):</span>
<span class="sd">                Optional. The name of the model (e.g. &quot;gemini-1.0-pro&quot;).</span>
<span class="sd">            prompt (langchain_core.runnables.RunnableSerializable):</span>
<span class="sd">                Optional. The prompt template for the model. Defaults to a</span>
<span class="sd">                ChatPromptTemplate.</span>
<span class="sd">            tools (Sequence[langchain_core.tools.BaseTool, Callable]):</span>
<span class="sd">                Optional. The tools for the agent to be able to use. All input</span>
<span class="sd">                callables (e.g. function or class method) will be converted</span>
<span class="sd">                to a langchain.tools.base.StructuredTool. Defaults to None.</span>
<span class="sd">            output_parser (langchain_core.runnables.RunnableSerializable):</span>
<span class="sd">                Optional. The output parser for the model. Defaults to an</span>
<span class="sd">                output parser that works with Gemini function-calling.</span>
<span class="sd">            chat_history (langchain_core.runnables.history.GetSessionHistoryCallable):</span>
<span class="sd">                Optional. Callable that returns a new BaseChatMessageHistory.</span>
<span class="sd">                Defaults to None, i.e. chat_history is not preserved.</span>
<span class="sd">            model_kwargs (Mapping[str, Any]):</span>
<span class="sd">                Optional. Additional keyword arguments for the constructor of</span>
<span class="sd">                chat_models.ChatVertexAI. An example would be</span>
<span class="sd">                ```</span>
<span class="sd">                {</span>
<span class="sd">                    # temperature (float): Sampling temperature, it controls the</span>
<span class="sd">                    # degree of randomness in token selection.</span>
<span class="sd">                    &quot;temperature&quot;: 0.28,</span>
<span class="sd">                    # max_output_tokens (int): Token limit determines the</span>
<span class="sd">                    # maximum amount of text output from one prompt.</span>
<span class="sd">                    &quot;max_output_tokens&quot;: 1000,</span>
<span class="sd">                    # top_p (float): Tokens are selected from most probable to</span>
<span class="sd">                    # least, until the sum of their probabilities equals the</span>
<span class="sd">                    # top_p value.</span>
<span class="sd">                    &quot;top_p&quot;: 0.95,</span>
<span class="sd">                    # top_k (int): How the model selects tokens for output, the</span>
<span class="sd">                    # next token is selected from among the top_k most probable</span>
<span class="sd">                    # tokens.</span>
<span class="sd">                    &quot;top_k&quot;: 40,</span>
<span class="sd">                }</span>
<span class="sd">                ```</span>
<span class="sd">            agent_executor_kwargs (Mapping[str, Any]):</span>
<span class="sd">                Optional. Additional keyword arguments for the constructor of</span>
<span class="sd">                langchain.agents.AgentExecutor. An example would be</span>
<span class="sd">                ```</span>
<span class="sd">                {</span>
<span class="sd">                    # Whether to return the agent&#39;s trajectory of intermediate</span>
<span class="sd">                    # steps at the end in addition to the final output.</span>
<span class="sd">                    &quot;return_intermediate_steps&quot;: False,</span>
<span class="sd">                    # The maximum number of steps to take before ending the</span>
<span class="sd">                    # execution loop.</span>
<span class="sd">                    &quot;max_iterations&quot;: 15,</span>
<span class="sd">                    # The method to use for early stopping if the agent never</span>
<span class="sd">                    # returns `AgentFinish`. Either &#39;force&#39; or &#39;generate&#39;.</span>
<span class="sd">                    &quot;early_stopping_method&quot;: &quot;force&quot;,</span>
<span class="sd">                    # How to handle errors raised by the agent&#39;s output parser.</span>
<span class="sd">                    # Defaults to `False`, which raises the error.</span>
<span class="sd">                    &quot;handle_parsing_errors&quot;: False,</span>
<span class="sd">                }</span>
<span class="sd">                ```</span>
<span class="sd">            runnable_kwargs (Mapping[str, Any]):</span>
<span class="sd">                Optional. Additional keyword arguments for the constructor of</span>
<span class="sd">                langchain.runnables.history.RunnableWithMessageHistory if</span>
<span class="sd">                chat_history is specified. If chat_history is None, this will be</span>
<span class="sd">                ignored.</span>
<span class="sd">            model_builder (Callable):</span>
<span class="sd">                Optional. Callable that returns a new language model. Defaults</span>
<span class="sd">                to a a callable that returns ChatVertexAI based on `model`,</span>
<span class="sd">                `model_kwargs` and the parameters in `vertexai.init`.</span>
<span class="sd">            runnable_builder (Callable):</span>
<span class="sd">                Optional. Callable that returns a new runnable. This can be used</span>
<span class="sd">                for customizing the orchestration logic of the Agent based on</span>
<span class="sd">                the model returned by `model_builder` and the rest of the input</span>
<span class="sd">                arguments.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If there is an invalid tool (e.g. function with an input</span>
<span class="sd">            that did not specify its type).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_project</span> <span class="o">=</span> <span class="n">initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">project</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_location</span> <span class="o">=</span> <span class="n">initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">location</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tools</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">tools</span><span class="p">:</span>
            <span class="c1"># We validate tools at initialization for actionable feedback before</span>
            <span class="c1"># they are deployed.</span>
            <span class="n">_validate_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tools</span> <span class="o">=</span> <span class="n">tools</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_name</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prompt</span> <span class="o">=</span> <span class="n">prompt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_parser</span> <span class="o">=</span> <span class="n">output_parser</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_chat_history</span> <span class="o">=</span> <span class="n">chat_history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_kwargs</span> <span class="o">=</span> <span class="n">model_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_agent_executor_kwargs</span> <span class="o">=</span> <span class="n">agent_executor_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_runnable_kwargs</span> <span class="o">=</span> <span class="n">runnable_kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_builder</span> <span class="o">=</span> <span class="n">model_builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_runnable</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_runnable_builder</span> <span class="o">=</span> <span class="n">runnable_builder</span>

<div class="viewcode-block" id="LangchainAgent.set_up"><a class="viewcode-back" href="../../../../../vertexai/services.html#vertexai.preview.reasoning_engines.LangchainAgent.set_up">[docs]</a>    <span class="k">def</span> <span class="nf">set_up</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets up the agent for execution of queries at runtime.</span>

<span class="sd">        It initializes the model, binds the model with tools, and connects it</span>
<span class="sd">        with the prompt template and output parser.</span>

<span class="sd">        This method should not be called for an object that being passed to</span>
<span class="sd">        the ReasoningEngine service for deployment, as it initializes clients</span>
<span class="sd">        that can not be serialized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_builder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_builder</span> <span class="ow">or</span> <span class="n">_default_model_builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model_builder</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_name</span><span class="p">,</span>
            <span class="n">model_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_kwargs</span><span class="p">,</span>
            <span class="n">project</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_project</span><span class="p">,</span>
            <span class="n">location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_location</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">runnable_builder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_runnable_builder</span> <span class="ow">or</span> <span class="n">_default_runnable_builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_runnable</span> <span class="o">=</span> <span class="n">runnable_builder</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prompt</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_tools</span><span class="p">,</span>
            <span class="n">output_parser</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_parser</span><span class="p">,</span>
            <span class="n">chat_history</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_chat_history</span><span class="p">,</span>
            <span class="n">agent_executor_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_agent_executor_kwargs</span><span class="p">,</span>
            <span class="n">runnable_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_runnable_kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="LangchainAgent.query"><a class="viewcode-back" href="../../../../../vertexai/services.html#vertexai.preview.reasoning_engines.LangchainAgent.query">[docs]</a>    <span class="k">def</span> <span class="nf">query</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;RunnableConfig&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Queries the Agent with the given input and config.</span>

<span class="sd">        Args:</span>
<span class="sd">            input (Union[str, Mapping[str, Any]]):</span>
<span class="sd">                Required. The input to be passed to the Agent.</span>
<span class="sd">            config (langchain_core.runnables.RunnableConfig):</span>
<span class="sd">                Optional. The config (if any) to be used for invoking the Agent.</span>
<span class="sd">            **kwargs:</span>
<span class="sd">                Optional. Any additional keyword arguments to be passed to the</span>
<span class="sd">                `.invoke()` method of the corresponding AgentExecutor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The output of querying the Agent with the given input and config.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">langchain.load</span> <span class="kn">import</span> <span class="n">dump</span> <span class="k">as</span> <span class="n">langchain_load_dump</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">}</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_runnable</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_up</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">langchain_load_dump</span><span class="o">.</span><span class="n">dumpd</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_runnable</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="p">)</span></div></div>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../index.html">google-cloud-aiplatform</a></h1>



<p class="blurb">Google Cloud Client Libraries for google-cloud-aiplatform</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=googleapis&repo=python-aiplatform&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../vertexai/services.html">Vertex AI SDK</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2019, Google.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
    </div>

    
    <a href="https://github.com/googleapis/python-aiplatform" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>