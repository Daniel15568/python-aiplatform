

<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>vertexai.preview.evaluation._eval_tasks &#8212; google-cloud-aiplatform  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>

  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
          	<div class="admonition" id="python2-eol"> 
          	 As of January 1, 2020 this library no longer supports Python 2 on the latest released version. 
          	 Library versions released prior to that date will continue to be available. For more information please
          	 visit <a href="https://cloud.google.com/python/docs/python2-sunset/">Python 2 support on Google Cloud</a>.
          	</div>
            
  <h1>Source code for vertexai.preview.evaluation._eval_tasks</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Copyright 2024 Google LLC</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">uuid</span>

<span class="kn">from</span> <span class="nn">google.api_core</span> <span class="kn">import</span> <span class="n">exceptions</span>
<span class="kn">import</span> <span class="nn">vertexai</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform.metadata</span> <span class="kn">import</span> <span class="n">metadata</span>
<span class="kn">from</span> <span class="nn">vertexai</span> <span class="kn">import</span> <span class="n">generative_models</span>
<span class="kn">from</span> <span class="nn">vertexai.preview.evaluation</span> <span class="kn">import</span> <span class="n">_base</span> <span class="k">as</span> <span class="n">eval_base</span>
<span class="kn">from</span> <span class="nn">vertexai.preview.evaluation</span> <span class="kn">import</span> <span class="n">_evaluation</span>
<span class="kn">from</span> <span class="nn">vertexai.preview.evaluation</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">vertexai.preview.evaluation.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_base</span> <span class="k">as</span> <span class="n">metrics_base</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># pylint: disable=g-import-not-at-top</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span> <span class="k">as</span> <span class="n">IPython_display</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">IPython_display</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">_LOGGER</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">EvalResult</span> <span class="o">=</span> <span class="n">eval_base</span><span class="o">.</span><span class="n">EvalResult</span>
<span class="n">GenerativeModel</span> <span class="o">=</span> <span class="n">generative_models</span><span class="o">.</span><span class="n">GenerativeModel</span>


<div class="viewcode-block" id="EvalTask"><a class="viewcode-back" href="../../../../vertexai/services.html#vertexai.preview.evaluation.EvalTask">[docs]</a><span class="k">class</span> <span class="nc">EvalTask</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A class representing an EvalTask.</span>

<span class="sd">    An Evaluation Tasks is defined to measure the model&#39;s ability to perform a</span>
<span class="sd">    certain task in response to specific prompts or inputs. Evaluation tasks must</span>
<span class="sd">    contain an evaluation dataset, and a list of metrics to evaluate. Evaluation</span>
<span class="sd">    tasks help developers compare propmpt templates, track experiments, compare</span>
<span class="sd">    models and their settings, and assess the quality of the model&#39;s generated</span>
<span class="sd">    text.</span>

<span class="sd">    Dataset details:</span>
<span class="sd">        Default dataset column names:</span>
<span class="sd">            * content_column_name: &quot;content&quot;</span>
<span class="sd">            * reference_column_name: &quot;reference&quot;</span>
<span class="sd">            * response_column_name: &quot;response&quot;</span>
<span class="sd">        Requirement for different use cases:</span>
<span class="sd">          * Bring your own prediction: A `response` column is required. Response</span>
<span class="sd">              column name can be customized by providing `response_column_name`</span>
<span class="sd">              parameter.</span>
<span class="sd">          * Without prompt template: A column representing the input prompt to the</span>
<span class="sd">              model is required. If `content_column_name` is not specified, the</span>
<span class="sd">              eval dataset requires `content` column by default. The response</span>
<span class="sd">              column is not used if present and new responses from the model are</span>
<span class="sd">              generated with the content column and used for evaluation.</span>
<span class="sd">          * With prompt template: Dataset must contain column names corresponding to</span>
<span class="sd">              the placeholder names in the prompt template. For example, if prompt</span>
<span class="sd">              template is &quot;Instruction: {instruction}, context: {context}&quot;, the</span>
<span class="sd">              dataset must contain `instruction` and `context` column.</span>

<span class="sd">    Metrics Details:</span>
<span class="sd">        The supported metrics, metric bundle descriptions, grading rubrics, and</span>
<span class="sd">        the required input fields can be found on the Vertex AI public</span>
<span class="sd">        documentation.</span>

<span class="sd">    Usage:</span>
<span class="sd">        1. To perform bring your own prediction evaluation, provide the model</span>
<span class="sd">        responses in the response column in the dataset. The response column name</span>
<span class="sd">        is &quot;response&quot; by default, or specify `response_column_name` parameter to</span>
<span class="sd">        customize.</span>

<span class="sd">          ```</span>
<span class="sd">          eval_dataset = pd.DataFrame({</span>
<span class="sd">                  &quot;reference&quot;: [...],</span>
<span class="sd">                  &quot;response&quot; : [...],</span>
<span class="sd">          })</span>
<span class="sd">          eval_task = EvalTask(</span>
<span class="sd">            dataset=eval_dataset,</span>
<span class="sd">            metrics=[&quot;bleu&quot;, &quot;rouge_l_sum&quot;, &quot;coherence&quot;, &quot;fluency&quot;],</span>
<span class="sd">            experiment=&quot;my-experiment&quot;,</span>
<span class="sd">          )</span>
<span class="sd">          eval_result = eval_task.evaluate(</span>
<span class="sd">                experiment_run_name=&quot;eval-experiment-run&quot;</span>
<span class="sd">          )</span>
<span class="sd">          ```</span>

<span class="sd">        2. To perform evaluation with built-in Gemini model inference, specify the</span>
<span class="sd">        `model` parameter with a GenerativeModel instance.  The default query</span>
<span class="sd">        column name to the model is `content`.</span>

<span class="sd">          ```</span>
<span class="sd">          eval_dataset = pd.DataFrame({</span>
<span class="sd">                &quot;reference&quot;: [...],</span>
<span class="sd">                &quot;content&quot;  : [...],</span>
<span class="sd">          })</span>
<span class="sd">          result = EvalTask(</span>
<span class="sd">              dataset=eval_dataset,</span>
<span class="sd">              metrics=[&quot;exact_match&quot;, &quot;bleu&quot;, &quot;rouge_1&quot;, &quot;rouge_2&quot;,</span>
<span class="sd">              &quot;rouge_l_sum&quot;],</span>
<span class="sd">              experiment=&quot;my-experiment&quot;,</span>
<span class="sd">          ).evaluate(</span>
<span class="sd">              model=GenerativeModel(&quot;gemini-pro&quot;),</span>
<span class="sd">              experiment_run_name=&quot;gemini-pro-eval-run&quot;</span>
<span class="sd">          )</span>
<span class="sd">          ```</span>

<span class="sd">        3. If a `prompt_template` is specified, the `content` column is not required.</span>
<span class="sd">        Prompts can be assembled from the evaluation dataset, and all placeholder</span>
<span class="sd">        names must be present in the dataset columns.</span>
<span class="sd">          ```</span>
<span class="sd">          eval_dataset = pd.DataFrame({</span>
<span class="sd">              &quot;context&quot;    : [...],</span>
<span class="sd">              &quot;instruction&quot;: [...],</span>
<span class="sd">              &quot;reference&quot;  : [...],</span>
<span class="sd">          })</span>
<span class="sd">          result = EvalTask(</span>
<span class="sd">              dataset=eval_dataset,</span>
<span class="sd">              metrics=[&quot;summarization_quality&quot;],</span>
<span class="sd">          ).evaluate(</span>
<span class="sd">              model=model,</span>
<span class="sd">              prompt_template=&quot;{instruction}. Article: {context}. Summary:&quot;,</span>
<span class="sd">          )</span>
<span class="sd">          ```</span>

<span class="sd">        4. To perform evaluation with custom model inference, specify the `model`</span>
<span class="sd">        parameter with a custom prediction function. The `content` column in the</span>
<span class="sd">        dataset is used to generate predictions with the custom model function for</span>
<span class="sd">        evaluation.</span>

<span class="sd">          ```</span>
<span class="sd">          def custom_model_fn(input: str) -&gt; str:</span>
<span class="sd">            response = client.chat.completions.create(</span>
<span class="sd">              model=&quot;gpt-3.5-turbo&quot;,</span>
<span class="sd">              messages=[</span>
<span class="sd">                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input}</span>
<span class="sd">              ]</span>
<span class="sd">            )</span>
<span class="sd">            return response.choices[0].message.content</span>

<span class="sd">          eval_dataset = pd.DataFrame({</span>
<span class="sd">                &quot;content&quot;  : [...],</span>
<span class="sd">                &quot;reference&quot;: [...],</span>
<span class="sd">          })</span>
<span class="sd">          result = EvalTask(</span>
<span class="sd">              dataset=eval_dataset,</span>
<span class="sd">              metrics=[&quot;text_generation_similarity&quot;,&quot;text_generation_quality&quot;],</span>
<span class="sd">              experiment=&quot;my-experiment&quot;,</span>
<span class="sd">          ).evaluate(</span>
<span class="sd">              model=custom_model_fn,</span>
<span class="sd">              experiment_run_name=&quot;gpt-eval-run&quot;</span>
<span class="sd">          )</span>
<span class="sd">          ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_resource_noun</span> <span class="o">=</span> <span class="s2">&quot;evalTasks&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;pd.DataFrame&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Literal</span><span class="p">[</span>
                    <span class="s2">&quot;exact_match&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;bleu&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;rouge_1&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;rouge_2&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;rouge_l&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;rouge_l_sum&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;coherence&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;fluency&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;safety&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;groundedness&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;fulfillment&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;summarization_quality&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;summarization_helpfulness&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;summarization_verbosity&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;question_answering_quality&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;question_answering_relevance&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;question_answering_helpfulness&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;question_answering_correctness&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;text_generation_similarity&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;text_generation_quality&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;text_generation_instruction_following&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;text_generation_safety&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;text_generation_factuality&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;summarization_pointwise_reference_free&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;qa_pointwise_reference_free&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;qa_pointwise_reference_based&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;tool_call_quality&quot;</span><span class="p">,</span>
                <span class="p">],</span>
                <span class="n">metrics_base</span><span class="o">.</span><span class="n">CustomMetric</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">],</span>
        <span class="n">experiment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">content_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;content&quot;</span><span class="p">,</span>
        <span class="n">reference_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reference&quot;</span><span class="p">,</span>
        <span class="n">response_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;response&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an EvalTask.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset: The dataset to be evaluated.</span>
<span class="sd">                Supports the following dataset formats:</span>
<span class="sd">                * pandas.DataFrame: Used directly for evaluation.</span>
<span class="sd">                * Dict: Converted to a pandas DataFrame before evaluation.</span>
<span class="sd">                * str: Interpreted as a file path or URI. Supported formats include:</span>
<span class="sd">                    * Local JSONL or CSV files:  Loaded from the local filesystem.</span>
<span class="sd">                    * GCS JSONL or CSV files: Loaded from Google Cloud Storage</span>
<span class="sd">                        (e.g., &#39;gs://bucket/data.csv&#39;).</span>
<span class="sd">                    * BigQuery table URI: Loaded from Google Cloud BigQuery</span>
<span class="sd">                        (e.g., &#39;bq://project-id.dataset.table_name&#39;).</span>
<span class="sd">            metrics: The list of metrics names to be evaluated, or a metrics</span>
<span class="sd">                bundle for an evaluation task, or custom metric instances.</span>
<span class="sd">            experiment: The name of the experiment to log the evaluations to.</span>
<span class="sd">            content_column_name: The column name of content in the dataset to send to</span>
<span class="sd">                the model. If not set, default to `content`.</span>
<span class="sd">            reference_column_name: The column name of ground truth in the dataset. If</span>
<span class="sd">                not set, default to `reference`.</span>
<span class="sd">            response_column_name: The column name of model response in the dataset. If</span>
<span class="sd">                not set, default to `response`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="o">=</span> <span class="n">experiment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content_column_name</span> <span class="o">=</span> <span class="n">content_column_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reference_column_name</span> <span class="o">=</span> <span class="n">reference_column_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">response_column_name</span> <span class="o">=</span> <span class="n">response_column_name</span>

    <span class="k">def</span> <span class="nf">_evaluate_with_experiment</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">GenerativeModel</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">experiment_run_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">response_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;response&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvalResult</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs an evaluation for the EvalTask with an experiment.</span>

<span class="sd">        Args:</span>
<span class="sd">          model: A GenerativeModel instance or a custom model function to generate</span>
<span class="sd">            responses to evaluate. If not provided, the evaluation is computed with</span>
<span class="sd">            the `response` column in the `dataset`.</span>
<span class="sd">          prompt_template: The prompt template to use for the evaluation. If not</span>
<span class="sd">            set, the prompt template that was used to create the EvalTask will be</span>
<span class="sd">            used.</span>
<span class="sd">          experiment_run_name: The name of the experiment run to log the evaluation</span>
<span class="sd">            to if an experiment is set for this EvalTask. If not provided, a random</span>
<span class="sd">            unique experiment run name is used.</span>
<span class="sd">          response_column_name: The column name of model response in the dataset. If</span>
<span class="sd">            not set, default to `response`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          The evaluation result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_experiment_run</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">vertexai</span><span class="o">.</span><span class="n">preview</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">experiment_run_name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_eval_experiment_param</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prompt_template</span><span class="p">)</span>
            <span class="n">eval_result</span> <span class="o">=</span> <span class="n">_evaluation</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">prompt_template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
                <span class="n">content_column_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">content_column_name</span><span class="p">,</span>
                <span class="n">reference_column_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_column_name</span><span class="p">,</span>
                <span class="n">response_column_name</span><span class="o">=</span><span class="n">response_column_name</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_column_name</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">vertexai</span><span class="o">.</span><span class="n">preview</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">eval_result</span><span class="o">.</span><span class="n">summary_metrics</span><span class="p">)</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">,</span> <span class="n">exceptions</span><span class="o">.</span><span class="n">InvalidArgument</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Experiment metrics logging failed: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_result</span>

<div class="viewcode-block" id="EvalTask.evaluate"><a class="viewcode-back" href="../../../../vertexai/services.html#vertexai.preview.evaluation.EvalTask.evaluate">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">GenerativeModel</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">experiment_run_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">response_column_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;response&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EvalResult</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs an evaluation for the EvalTask.</span>

<span class="sd">        Args:</span>
<span class="sd">          model: A GenerativeModel instance or a custom model function to generate</span>
<span class="sd">            responses to evaluate. If not provided, the evaluation is computed with</span>
<span class="sd">            the `response` column in the `dataset`.</span>
<span class="sd">          prompt_template: The prompt template to use for the evaluation. If not</span>
<span class="sd">            set, the prompt template that was used to create the EvalTask will be</span>
<span class="sd">            used.</span>
<span class="sd">          experiment_run_name: The name of the experiment run to log the evaluation</span>
<span class="sd">            to if an experiment is set for this EvalTask. If not provided, a random</span>
<span class="sd">            unique experiment run name is used.</span>
<span class="sd">          response_column_name: The column name of model response in the dataset. If</span>
<span class="sd">            not set, default to `response`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          The evaluation result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">global_experiment_name</span> <span class="o">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">_experiment_tracker</span><span class="o">.</span><span class="n">experiment_name</span>
        <span class="k">if</span> <span class="n">experiment_run_name</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">global_experiment_name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Experiment is not set. Please initialize EvalTask with an&quot;</span>
                <span class="s2">&quot; experiment, or initialize a global experiment with &quot;</span>
                <span class="s2">&quot;`vertexai.init(experiment=&#39;experiment_name&#39;)`for logging this&quot;</span>
                <span class="s2">&quot; evaluation run.&quot;</span>
            <span class="p">)</span>

        <span class="n">experiment_run_name</span> <span class="o">=</span> <span class="n">experiment_run_name</span> <span class="ow">or</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="ow">and</span> <span class="n">global_experiment_name</span><span class="p">:</span>
            <span class="n">metadata</span><span class="o">.</span><span class="n">_experiment_tracker</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span>
                <span class="n">experiment</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">experiment</span><span class="p">,</span> <span class="n">backing_tensorboard</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">eval_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_experiment</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">prompt_template</span><span class="p">,</span> <span class="n">experiment_run_name</span><span class="p">,</span> <span class="n">response_column_name</span>
            <span class="p">)</span>
            <span class="n">metadata</span><span class="o">.</span><span class="n">_experiment_tracker</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span>
                <span class="n">experiment</span><span class="o">=</span><span class="n">global_experiment_name</span><span class="p">,</span> <span class="n">backing_tensorboard</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">global_experiment_name</span><span class="p">:</span>
            <span class="n">metadata</span><span class="o">.</span><span class="n">_experiment_tracker</span><span class="o">.</span><span class="n">set_experiment</span><span class="p">(</span>
                <span class="n">experiment</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">experiment</span><span class="p">,</span> <span class="n">backing_tensorboard</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">eval_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_experiment</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">prompt_template</span><span class="p">,</span> <span class="n">experiment_run_name</span><span class="p">,</span> <span class="n">response_column_name</span>
            <span class="p">)</span>
            <span class="n">metadata</span><span class="o">.</span><span class="n">_experiment_tracker</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="ow">and</span> <span class="n">global_experiment_name</span><span class="p">:</span>
            <span class="n">eval_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_experiment</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">prompt_template</span><span class="p">,</span> <span class="n">experiment_run_name</span><span class="p">,</span> <span class="n">response_column_name</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_result</span> <span class="o">=</span> <span class="n">_evaluation</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">prompt_template</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span>
                <span class="n">content_column_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">content_column_name</span><span class="p">,</span>
                <span class="n">reference_column_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_column_name</span><span class="p">,</span>
                <span class="n">response_column_name</span><span class="o">=</span><span class="n">response_column_name</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_column_name</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_result</span></div>

    <span class="k">def</span> <span class="nf">_validate_experiment_run</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Checks if an experiment run already exists.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">metadata</span><span class="o">.</span><span class="n">_experiment_tracker</span><span class="o">.</span><span class="n">experiment_run</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Experiment run already exists. Please specify the name of the&quot;</span>
                <span class="s2">&quot; experiment run to assign current session with in this evaluate&quot;</span>
                <span class="s2">&quot; method.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_eval_experiment_param</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">GenerativeModel</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_template</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Logs variable input parameters of an evaluation to an experiment run.&quot;&quot;&quot;</span>
        <span class="n">model_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">prompt_template</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">model_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;prompt_template&quot;</span><span class="p">:</span> <span class="n">prompt_template</span><span class="p">})</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">GenerativeModel</span><span class="p">):</span>
            <span class="n">model_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">_model_name</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">_generation_config</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_generation_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="c1"># TODO(b/311221071): support logging GenerationConfig type.</span>
                <span class="n">model_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">**</span><span class="n">model</span><span class="o">.</span><span class="n">_generation_config</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">_safety_settings</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_safety_settings</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="c1"># TODO(b/311221071): support logging List[SafetySetting] type.</span>
                <span class="n">safety_settings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_safety_settings</span>
                <span class="n">safety_settings_as_str</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">category</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">threshold</span><span class="o">.</span><span class="n">name</span>
                    <span class="k">for</span> <span class="n">category</span><span class="p">,</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">safety_settings</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="p">}</span>
                <span class="n">model_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">safety_settings_as_str</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">model_metadata</span><span class="p">:</span>
            <span class="n">_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logging Rapid Eval experiment metadata: </span><span class="si">{</span><span class="n">model_metadata</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">vertexai</span><span class="o">.</span><span class="n">preview</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="n">model_metadata</span><span class="p">)</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Experiment metadata logging failed: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="EvalTask.display_runs"><a class="viewcode-back" href="../../../../vertexai/services.html#vertexai.preview.evaluation.EvalTask.display_runs">[docs]</a>    <span class="k">def</span> <span class="nf">display_runs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Displays experiment runs associated with this EvalTask.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Experiment is not set.&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">IPython_display</span><span class="p">:</span>
            <span class="n">IPython_display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">vertexai</span><span class="o">.</span><span class="n">preview</span><span class="o">.</span><span class="n">get_experiment_df</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">experiment</span><span class="p">))</span></div></div>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">google-cloud-aiplatform</a></h1>



<p class="blurb">Google Cloud Client Libraries for google-cloud-aiplatform</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=googleapis&repo=python-aiplatform&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../vertexai/services.html">Vertex AI SDK</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2019, Google.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
    </div>

    
    <a href="https://github.com/googleapis/python-aiplatform" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>