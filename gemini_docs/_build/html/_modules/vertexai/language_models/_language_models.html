

<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>vertexai.language_models._language_models &#8212; google-cloud-aiplatform  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>

  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
          	<div class="admonition" id="python2-eol"> 
          	 As of January 1, 2020 this library no longer supports Python 2 on the latest released version. 
          	 Library versions released prior to that date will continue to be available. For more information please
          	 visit <a href="https://cloud.google.com/python/docs/python2-sunset/">Python 2 support on Google Cloud</a>.
          	</div>
            
  <h1>Source code for vertexai.language_models._language_models</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023 Google LLC</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="sd">&quot;&quot;&quot;Classes for working with language models.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">collections.abc</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">AsyncIterator</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Literal</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">aiplatform</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">_streaming_prediction</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">initializer</span> <span class="k">as</span> <span class="n">aiplatform_initializer</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform</span> <span class="kn">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">aiplatform_utils</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform.compat</span> <span class="kn">import</span> <span class="n">types</span> <span class="k">as</span> <span class="n">aiplatform_types</span>
<span class="kn">from</span> <span class="nn">google.cloud.aiplatform.utils</span> <span class="kn">import</span> <span class="n">gcs_utils</span>
<span class="kn">from</span> <span class="nn">vertexai._model_garden</span> <span class="kn">import</span> <span class="n">_model_garden_models</span>
<span class="kn">from</span> <span class="nn">vertexai.language_models</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_evaluatable_language_models</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">pandas</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">pandas</span> <span class="o">=</span> <span class="kc">None</span>


<span class="n">_LOGGER</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="c1"># Endpoint label/metadata key to preserve the base model ID information</span>
<span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="o">=</span> <span class="s2">&quot;google-vertex-llm-tuning-base-model-id&quot;</span>

<span class="c1"># Default URI for RLHF training template in the Vertex Template Gallery</span>
<span class="n">_DEFAULT_RLHF_TUNING_PIPELINE_URI</span> <span class="o">=</span> <span class="s2">&quot;https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/rlhf-train-template/default&quot;</span>

<span class="n">_ACCELERATOR_TYPES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;TPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">]</span>
<span class="n">_ACCELERATOR_TYPE_TYPE</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;TPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets the base model ID for the model ID labels used the tuned models.</span>

<span class="sd">    Args:</span>
<span class="sd">        tuning_model_id: The model ID used in tuning. E.g. `text-bison-001`</span>

<span class="sd">    Returns:</span>
<span class="sd">        The publisher model ID</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If tuning model ID is unsupported</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">version</span> <span class="o">=</span> <span class="n">tuning_model_id</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
    <span class="c1"># &quot;publishers/google/models/text-bison@001&quot;</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;publishers/google/models/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">@</span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">def</span> <span class="nf">_get_tensorboard_resource_id_from_evaluation_spec</span><span class="p">(</span>
    <span class="n">eval_spec</span><span class="p">:</span> <span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets the Tensorboard resource ID from an evaluation spec.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_spec: To extract Tensorboard resource ID from.</span>
<span class="sd">        tuning_job_location: Location . Used to check that the tensorboard</span>
<span class="sd">            location is in the same region as tuning.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensorboard resource ID, if it was set.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If Tensorboard location is not in the same region as tuning</span>
<span class="sd">        TypeError: If Tensorboard URI is not a string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span><span class="p">,</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Tensorboard</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span><span class="o">.</span><span class="n">location</span> <span class="o">!=</span> <span class="n">tuning_job_location</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The Tensorboard must be in the same location as the tuning job.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span><span class="o">.</span><span class="n">resource_name</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">resource_name_parts</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Tensorboard</span><span class="o">.</span><span class="n">_parse_resource_name</span><span class="p">(</span>
            <span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">resource_name_parts</span><span class="p">[</span><span class="s2">&quot;location&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">tuning_job_location</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The Tensorboard must be in the same location as the tuning job. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Tensorboard location: </span><span class="si">{</span><span class="n">resource_name_parts</span><span class="p">[</span><span class="s1">&#39;location&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;tuning job location: </span><span class="si">{</span><span class="n">tuning_job_location</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">tensorboard</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Tensorboard should be a URI string&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_LanguageModel</span><span class="p">(</span><span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_ModelGardenModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;_LanguageModel is a base class for all language models.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a LanguageModel.</span>

<span class="sd">        This constructor should not be called directly.</span>
<span class="sd">        Use `LanguageModel.from_pretrained(model_name=...)` instead.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_id: Identifier of a Vertex LLM. Example: &quot;text-bison@001&quot;</span>
<span class="sd">            endpoint_name: Vertex Endpoint resource name for the model</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_model_resource_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Full resource name of the model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;publishers/&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># This is a ModelRegistry resource name</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">list_models</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">_PredictionRequest</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A single-instance prediction request.&quot;&quot;&quot;</span>

    <span class="n">instance</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">_MultiInstancePredictionRequest</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A multi-instance prediction request.&quot;&quot;&quot;</span>

    <span class="n">instances</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">_GetTunedModelMixin</span><span class="p">(</span><span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_ModelGardenModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mixin that adds methods that list and get tuned language models.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">list_tuned_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Lists the names of tuned models.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of tuned models that can be used with the `get_tuned_model` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
            <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_list_tuned_model_names</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tuned_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the specified tuned language model.&quot;&quot;&quot;</span>

        <span class="n">tuned_vertex_model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">)</span>
        <span class="n">tuned_model_labels</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span>

        <span class="k">if</span> <span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_model_labels</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided model </span><span class="si">{</span><span class="n">tuned_model_name</span><span class="si">}</span><span class="s2"> does not have a base model ID.&quot;</span>
            <span class="p">)</span>

        <span class="n">tuning_model_id</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="p">]</span>

        <span class="n">tuned_model_deployments</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">deployed_models</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tuned_model_deployments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Deploying the model</span>
            <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">()</span><span class="o">.</span><span class="n">resource_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model_deployments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">endpoint</span>

        <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">)</span>
        <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
            <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="bp">cls</span><span class="p">},</span>
        <span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">interface_class</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>


<div class="viewcode-block" id="_TunableModelMixin"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models._TunableModelMixin">[docs]</a><span class="k">class</span> <span class="nc">_TunableModelMixin</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">,</span> <span class="n">_GetTunedModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model that can be tuned with supervised fine tuning (SFT).&quot;&quot;&quot;</span>

<div class="viewcode-block" id="_TunableModelMixin.tune_model"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models._TunableModelMixin.tune_model">[docs]</a>    <span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">corpus_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">queries_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">machine_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_context_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model(...)</span>
<span class="sd">        ... do some other work</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: A URI to training data in TSV (for embedding models), or JSON lines format, or a Pandas DataFrame.</span>
<span class="sd">            corpus_data: A URI to corpus in JSON lines format.</span>
<span class="sd">            queries_data: A URI to queries in JSON lines format.</span>
<span class="sd">            test_data: A URI to test data in TSV format.</span>
<span class="sd">            validation_data: A URI to validation data in TSV format.</span>
<span class="sd">            batch_size: Size of batch (for embedding models).</span>
<span class="sd">            train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">            learning_rate: Deprecated. Use learning_rate_multiplier instead.</span>
<span class="sd">                Learning rate to use in tuning.</span>
<span class="sd">            learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>
<span class="sd">            default_context: The context to use for all training samples by default.</span>
<span class="sd">            task_type: Type of task. Can be &quot;RETRIEVAL_QUERY&quot;, &quot;RETRIEVAL_DOCUMENT&quot;, &quot;SEMANTIC_SIMILARITY&quot;, &quot;CLASSIFICATION&quot;, &quot;CLUSTERING&quot;, &quot;QUESTION_ANSWERING&quot;, or &quot;FACT_VERIFICATION&quot;.</span>
<span class="sd">            machine_type: Machine type. E.g., &quot;a2-highgpu-1g&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator: Kind of accelerator. E.g., &quot;NVIDIA_TESLA_A100&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator_count: Count of accelerators.</span>
<span class="sd">            accelerator_type: Type of accelerator to use. Type can be &quot;TPU&quot; or &quot;GPU&quot;. Type is ignored, if accelerator is specified.</span>
<span class="sd">            max_context_length: The max context length used for tuning.</span>
<span class="sd">                Can be either &#39;8k&#39; or &#39;32k&#39;</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tuning_parameters</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="k">if</span> <span class="n">train_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;train_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_steps</span>
        <span class="k">if</span> <span class="n">learning_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_LOGGER</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The learning_rate parameter is deprecated.&quot;</span>
                <span class="s2">&quot;Use the learning_rate_multiplier parameter instead.&quot;</span>
            <span class="p">)</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="k">if</span> <span class="n">learning_rate_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;learning_rate_multiplier&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learning_rate_multiplier</span>
        <span class="n">eval_spec</span> <span class="o">=</span> <span class="n">tuning_evaluation_spec</span>
        <span class="k">if</span> <span class="n">eval_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">evaluation_data</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_spec</span><span class="o">.</span><span class="n">evaluation_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">evaluation_data</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
                        <span class="n">tuning_parameters</span><span class="p">[</span>
                            <span class="s2">&quot;evaluation_data_uri&quot;</span>
                        <span class="p">]</span> <span class="o">=</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">evaluation_data</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;evaluation_data should be a GCS URI&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;evaluation_data should be a URI string&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;evaluation_interval&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">evaluation_interval</span>
            <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">enable_early_stopping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tuning_parameters</span><span class="p">[</span>
                    <span class="s2">&quot;enable_early_stopping&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">enable_early_stopping</span>
            <span class="k">if</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">enable_checkpoint_selection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tuning_parameters</span><span class="p">[</span>
                    <span class="s2">&quot;enable_checkpoint_selection&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">eval_spec</span><span class="o">.</span><span class="n">enable_checkpoint_selection</span>
            <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="n">_get_tensorboard_resource_id_from_evaluation_spec</span><span class="p">(</span>
                <span class="n">eval_spec</span><span class="p">,</span> <span class="n">tuning_job_location</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">tensorboard_resource_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;tensorboard_resource_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensorboard_resource_id</span>

        <span class="k">if</span> <span class="n">default_context</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;default_context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_context</span>

        <span class="k">if</span> <span class="n">task_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;task_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">task_type</span>
        <span class="k">if</span> <span class="n">machine_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;machine_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">machine_type</span>
        <span class="k">if</span> <span class="n">accelerator_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;accelerator_count&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">accelerator_count</span>
        <span class="k">if</span> <span class="n">accelerator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;accelerator_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">accelerator</span>
        <span class="k">elif</span> <span class="n">accelerator_type</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">accelerator_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_ACCELERATOR_TYPES</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unsupported accelerator type: </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; Supported types: </span><span class="si">{</span><span class="n">_ACCELERATOR_TYPES</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;accelerator_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">accelerator_type</span>

        <span class="k">if</span> <span class="n">max_context_length</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;max_context_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_context_length</span>

        <span class="k">if</span> <span class="n">corpus_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;corpus_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">corpus_data</span>
        <span class="k">if</span> <span class="n">queries_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;queries_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">queries_data</span>
        <span class="k">if</span> <span class="n">test_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;test_label_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_data</span>
        <span class="k">if</span> <span class="n">validation_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;validation_label_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">validation_data</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tune_model</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">tuning_parameters</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="p">,</span>
            <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tuning_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches a model tuning job that can take some time.</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">                The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">            tuning_parameters: Tuning pipeline parameter values.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tuning_job_location</span> <span class="ow">and</span> <span class="n">tuning_job_location</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_TUNING_LOCATIONS</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="n">_get_invalid_tuning_location_msg</span><span class="p">(</span>
                    <span class="n">requested_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
                    <span class="n">valid_locations</span><span class="o">=</span><span class="n">_TUNING_LOCATIONS</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">tuned_model_location</span> <span class="ow">and</span> <span class="n">tuned_model_location</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_TUNED_MODEL_LOCATIONS</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Tuned model deployment is only supported in the following locations: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">_TUNED_MODEL_LOCATIONS</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
            <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">model_info</span><span class="o">.</span><span class="n">tuning_pipeline_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span>
            <span class="s2">&quot;https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model&quot;</span>
        <span class="p">):</span>
            <span class="n">train_steps</span> <span class="o">=</span> <span class="n">tuning_parameters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;train_steps&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">train_steps</span><span class="p">:</span>
                <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;iterations&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_steps</span>
            <span class="n">tunable_base_model_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;base_model_version_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tunable_base_model_id</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tuning_parameters</span><span class="p">[</span><span class="s2">&quot;large_model_reference&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span>
            <span class="k">if</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">encryption_spec_key_name</span><span class="p">:</span>
                <span class="n">tuning_parameters</span><span class="p">[</span>
                    <span class="s2">&quot;encryption_spec_key_name&quot;</span>
                <span class="p">]</span> <span class="o">=</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">encryption_spec_key_name</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_info</span><span class="o">.</span><span class="n">tuning_pipeline_uri</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="si">}</span><span class="s2"> model does not support tuning&quot;</span><span class="p">)</span>
        <span class="n">pipeline_job</span> <span class="o">=</span> <span class="n">_launch_tuning_job</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span><span class="p">,</span>
            <span class="n">tuning_pipeline_uri</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_pipeline_uri</span><span class="p">,</span>
            <span class="n">tuning_parameters</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bundle_up_tuning_job</span><span class="p">(</span><span class="n">pipeline_job</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_bundle_up_tuning_job</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pipeline_job</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LanguageModelTuningJob</span><span class="p">(</span>
            <span class="n">base_model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">job</span><span class="o">=</span><span class="n">pipeline_job</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">_RlhfTuningParameters</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configurable parameters for RLHF tuning.&quot;&quot;&quot;</span>

    <span class="n">prompt_dataset</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">preference_dataset</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">large_model_reference</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">prompt_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">target_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">reward_model_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">reinforcement_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">reward_model_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">reinforcement_learning_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">kl_coeff</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">instruction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">deploy_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">project</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">tensorboard_resource_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a dictionary of tuning parameters with undefined optional keys removed.&quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">asdict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">_RlhfTunableModelMixin</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">,</span> <span class="n">_GetTunedModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model that can be tuned with reinforcement learning from human feedback (RLHF).&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">tune_model_rlhf</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prompt_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="n">preference_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">target_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reward_model_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reinforcement_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reward_model_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">reinforcement_learning_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kl_coeff</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model using reinforcement learning from human feedback.</span>

<span class="sd">        This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model_rlhf(...)</span>
<span class="sd">        ... do some other work</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt_data: A Pandas DataFrame or a URI pointing to data in JSON lines</span>
<span class="sd">                format. The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</span>
<span class="sd">            preference_data: A Pandas DataFrame or a URI pointing to data in JSON lines</span>
<span class="sd">                format. The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">                If not provided, a default name will be created.</span>
<span class="sd">            prompt_sequence_length: Maximum tokenized sequence length for input text.</span>
<span class="sd">                Higher values increase memory overhead.</span>
<span class="sd">                This value should be at most 8192. Default value is 512.</span>
<span class="sd">            target_sequence_length: Maximum tokenized sequence length for target text.</span>
<span class="sd">                Higher values increase memory overhead.</span>
<span class="sd">                This value should be at most 1024. Default value is 64.</span>
<span class="sd">            reward_model_learning_rate_multiplier: Constant used to adjust the base</span>
<span class="sd">                learning rate used when training a reward model. Multiply by a</span>
<span class="sd">                number &gt; 1 to increase the magnitude of updates applied at each</span>
<span class="sd">                training step or multiply by a number &lt; 1 to decrease the magnitude</span>
<span class="sd">                of updates. Default value is 1.0.</span>
<span class="sd">            reinforcement_learning_rate_multiplier: Constant used to adjust the base</span>
<span class="sd">                learning rate used during reinforcement learning. Multiply by a</span>
<span class="sd">                number &gt; 1 to increase the magnitude of updates applied at each</span>
<span class="sd">                training step or multiply by a number &lt; 1 to decrease the magnitude</span>
<span class="sd">                of updates. Default value is 1.0.</span>
<span class="sd">            reward_model_train_steps: Number of steps to use when training a reward</span>
<span class="sd">                model. Default value is 1000.</span>
<span class="sd">            reinforcement_learning_train_steps: Number of reinforcement learning steps</span>
<span class="sd">                to perform when tuning a base model. Default value is 1000.</span>
<span class="sd">            kl_coeff: Coefficient for KL penalty. This regularizes the policy model and</span>
<span class="sd">                penalizes if it diverges from its initial distribution. If set to 0,</span>
<span class="sd">                the reference language model is not loaded into memory. Default value</span>
<span class="sd">                is 0.1.</span>
<span class="sd">            default_context: This field lets the model know what task to perform.</span>
<span class="sd">                Base models have been trained over a large set of varied instructions.</span>
<span class="sd">                You can give a simple and intuitive description of the task and the</span>
<span class="sd">                model will follow it, e.g. &quot;Classify this movie review as positive or</span>
<span class="sd">                negative&quot; or &quot;Translate this sentence to Danish&quot;. Do not specify this</span>
<span class="sd">                if your dataset already prepends the instruction to the inputs field.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">            tuning_evaluation_spec: Evaluation settings to use during tuning.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tuning_job_location</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tuning_job_location</span> <span class="ow">or</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">location</span>
        <span class="p">)</span>
        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">tuning_evaluation_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_unused_rlhf_eval_specs</span><span class="p">(</span><span class="n">tuning_evaluation_spec</span><span class="p">)</span>

            <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_data</span>
            <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;evaluation_data must be a GCS URI that starts with gs://&quot;</span>
                <span class="p">)</span>

            <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="n">_get_tensorboard_resource_id_from_evaluation_spec</span><span class="p">(</span>
                <span class="n">tuning_evaluation_spec</span><span class="p">,</span> <span class="n">tuning_job_location</span>
            <span class="p">)</span>
        <span class="n">prompt_dataset_uri</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">prompt_data</span><span class="p">,</span>
            <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">preference_dataset_uri</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">preference_data</span><span class="p">,</span>
            <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">accelerator_type</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">accelerator_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_ACCELERATOR_TYPES</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unsupported accelerator type: </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; Supported types: </span><span class="si">{</span><span class="n">_ACCELERATOR_TYPES</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="n">tuning_parameters</span> <span class="o">=</span> <span class="n">_RlhfTuningParameters</span><span class="p">(</span>
            <span class="n">prompt_dataset</span><span class="o">=</span><span class="n">prompt_dataset_uri</span><span class="p">,</span>
            <span class="n">preference_dataset</span><span class="o">=</span><span class="n">preference_dataset_uri</span><span class="p">,</span>
            <span class="n">large_model_reference</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">prompt_sequence_length</span><span class="o">=</span><span class="n">prompt_sequence_length</span><span class="p">,</span>
            <span class="n">target_sequence_length</span><span class="o">=</span><span class="n">target_sequence_length</span><span class="p">,</span>
            <span class="n">reward_model_learning_rate_multiplier</span><span class="o">=</span><span class="n">reward_model_learning_rate_multiplier</span><span class="p">,</span>
            <span class="n">reinforcement_learning_rate_multiplier</span><span class="o">=</span><span class="n">reinforcement_learning_rate_multiplier</span><span class="p">,</span>
            <span class="n">reward_model_train_steps</span><span class="o">=</span><span class="n">reward_model_train_steps</span><span class="p">,</span>
            <span class="n">reinforcement_learning_train_steps</span><span class="o">=</span><span class="n">reinforcement_learning_train_steps</span><span class="p">,</span>
            <span class="n">kl_coeff</span><span class="o">=</span><span class="n">kl_coeff</span><span class="p">,</span>
            <span class="n">instruction</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
            <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
            <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
            <span class="n">tensorboard_resource_id</span><span class="o">=</span><span class="n">tensorboard_resource_id</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tune_model_rlhf</span><span class="p">(</span>
            <span class="n">tuning_parameters</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_tune_model_rlhf</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tuning_parameters</span><span class="p">:</span> <span class="n">_RlhfTuningParameters</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model using reinforcement learning from human feedback.</span>

<span class="sd">        This method launches a tuning job that can take some time.</span>

<span class="sd">        Args:</span>
<span class="sd">            tuning_parameters: Tuning pipeline parameter values.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the tuning location is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tuning_parameters</span><span class="o">.</span><span class="n">location</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_TUNING_LOCATIONS</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="n">_get_invalid_tuning_location_msg</span><span class="p">(</span>
                    <span class="n">requested_location</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
                    <span class="n">valid_locations</span><span class="o">=</span><span class="n">_TUNING_LOCATIONS</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_SUPPORTED_RLHF_MODELS</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="n">_get_invalid_rlhf_model_msg</span><span class="p">(</span>
                    <span class="n">requested_model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">pipeline_job</span> <span class="o">=</span> <span class="n">_launch_rlhf_tuning_job</span><span class="p">(</span><span class="n">tuning_parameters</span><span class="p">)</span>

        <span class="n">job</span> <span class="o">=</span> <span class="n">_LanguageModelTuningJob</span><span class="p">(</span>
            <span class="n">base_model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">job</span><span class="o">=</span><span class="n">pipeline_job</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">job</span>


<span class="k">class</span> <span class="nc">_TunableTextModelMixin</span><span class="p">(</span><span class="n">_TunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Text model that can be tuned.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_context_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model(...)</span>
<span class="sd">        ... do some other work</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">                The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">            train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">            learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>
<span class="sd">            accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">            max_context_length: The max context length used for tuning.</span>
<span class="sd">                Can be either &#39;8k&#39; or &#39;32k&#39;</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note: Chat models do not support default_context</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
            <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
            <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">tuning_evaluation_spec</span><span class="o">=</span><span class="n">tuning_evaluation_spec</span><span class="p">,</span>
            <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
            <span class="n">max_context_length</span><span class="o">=</span><span class="n">max_context_length</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_PreviewTunableTextModelMixin</span><span class="p">(</span><span class="n">_TunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Text model that can be tuned.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_context_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches a model tuning job, waits for completion,</span>
<span class="sd">        updates the model in-place. This method returns job object for forward</span>
<span class="sd">        compatibility.</span>
<span class="sd">        In the future (GA), this method will become asynchronous and will stop</span>
<span class="sd">        updating the model in-place.</span>

<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model(...)  # Blocks until tuning is complete</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">                The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">            train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">            learning_rate: Deprecated. Use learning_rate_multiplier instead.</span>
<span class="sd">                Learning rate to use in tuning.</span>
<span class="sd">            learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>
<span class="sd">            accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">            max_context_length: The max context length used for tuning.</span>
<span class="sd">                Can be either &#39;8k&#39; or &#39;32k&#39;</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note: Chat models do not support default_context</span>
        <span class="n">job</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
            <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">tuning_evaluation_spec</span><span class="o">=</span><span class="n">tuning_evaluation_spec</span><span class="p">,</span>
            <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
            <span class="n">max_context_length</span><span class="o">=</span><span class="n">max_context_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">tuned_model</span> <span class="o">=</span> <span class="n">job</span><span class="o">.</span><span class="n">get_tuned_model</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">_endpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">_endpoint_name</span>
        <span class="k">return</span> <span class="n">job</span>


<span class="k">class</span> <span class="nc">_TunableChatModelMixin</span><span class="p">(</span><span class="n">_TunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Chat model that can be tuned.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model(...)</span>
<span class="sd">        ... do some other work</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">                The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">            train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">            learning_rate: Deprecated. Use learning_rate_multiplier instead.</span>
<span class="sd">                Learning rate to use in tuning.</span>
<span class="sd">            learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            default_context: The context to use for all training samples by default.</span>
<span class="sd">            accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">            tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">            AttributeError: If any attribute in the &quot;tuning_evaluation_spec&quot; is not supported</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">tuning_evaluation_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsupported_chat_model_tuning_eval_spec</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;evaluation_data&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_data</span><span class="p">,</span>
                <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_interval</span><span class="p">,</span>
                <span class="s2">&quot;enable_early_stopping&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">enable_early_stopping</span><span class="p">,</span>
                <span class="s2">&quot;enable_checkpoint_selection&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">enable_checkpoint_selection</span><span class="p">,</span>
            <span class="p">}</span>

            <span class="k">for</span> <span class="n">att_name</span><span class="p">,</span> <span class="n">att_value</span> <span class="ow">in</span> <span class="n">unsupported_chat_model_tuning_eval_spec</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">att_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;ChatModel and CodeChatModel only support tensorboard as attribute for TuningEvaluationSpec&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;found attribute name </span><span class="si">{</span><span class="n">att_name</span><span class="si">}</span><span class="s2"> with value </span><span class="si">{</span><span class="n">att_value</span><span class="si">}</span><span class="s2">, please leave </span><span class="si">{</span><span class="n">att_name</span><span class="si">}</span><span class="s2"> to None&quot;</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
            <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
            <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">default_context</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
            <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
            <span class="n">tuning_evaluation_spec</span><span class="o">=</span><span class="n">tuning_evaluation_spec</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_PreviewTunableChatModelMixin</span><span class="p">(</span><span class="n">_TunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Chat model that can be tuned.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches a model tuning job, waits for completion,</span>
<span class="sd">        updates the model in-place. This method returns job object for forward</span>
<span class="sd">        compatibility.</span>
<span class="sd">        In the future (GA), this method will become asynchronous and will stop</span>
<span class="sd">        updating the model in-place.</span>

<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model(...)  # Blocks until tuning is complete</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">        ```</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">                The dataset schema is model-specific.</span>
<span class="sd">                See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">            train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">            learning_rate: Deprecated. Use learning_rate_multiplier instead.</span>
<span class="sd">                Learning rate to use in tuning.</span>
<span class="sd">            learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">            tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            default_context: The context to use for all training samples by default.</span>
<span class="sd">            accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note: Chat models do not support tuning_evaluation_spec</span>
        <span class="n">job</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
            <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">default_context</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
            <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">tuned_model</span> <span class="o">=</span> <span class="n">job</span><span class="o">.</span><span class="n">get_tuned_model</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">_endpoint</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model</span><span class="o">.</span><span class="n">_endpoint_name</span>
        <span class="k">return</span> <span class="n">job</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">CountTokensResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The response from a count_tokens request.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        total_tokens (int):</span>
<span class="sd">            The total number of tokens counted across all</span>
<span class="sd">            instances passed to the request.</span>
<span class="sd">        total_billable_characters (int):</span>
<span class="sd">            The total number of billable characters</span>
<span class="sd">            counted across all instances from the request.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">total_tokens</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">total_billable_characters</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">_count_tokens_response</span><span class="p">:</span> <span class="n">Any</span>


<span class="k">class</span> <span class="nc">_CountTokensMixin</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mixin for models that support the CountTokens API&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CountTokensResponse</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Counts the tokens and billable characters for a given prompt.</span>

<span class="sd">        Note: this does not make a prediction request to the model, it only counts the tokens</span>
<span class="sd">        in the request.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts (List[str]):</span>
<span class="sd">                Required. A list of prompts to ask the model. For example: [&quot;What should I do today?&quot;, &quot;How&#39;s it going?&quot;]</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `CountTokensResponse` object that contains the number of tokens</span>
<span class="sd">            in the text and the number of billable characters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
            <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span>

        <span class="n">count_tokens_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span><span class="o">.</span><span class="n">select_version</span><span class="p">(</span>
            <span class="s2">&quot;v1beta1&quot;</span>
        <span class="p">)</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span>
            <span class="n">endpoint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instances</span><span class="o">=</span><span class="n">instances</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">CountTokensResponse</span><span class="p">(</span>
            <span class="n">total_tokens</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="o">.</span><span class="n">total_tokens</span><span class="p">,</span>
            <span class="n">total_billable_characters</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="o">.</span><span class="n">total_billable_characters</span><span class="p">,</span>
            <span class="n">_count_tokens_response</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="p">,</span>
        <span class="p">)</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TuningEvaluationSpec</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Specification for model evaluation to perform during tuning.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        evaluation_data: GCS URI of the evaluation dataset. This will run</span>
<span class="sd">            model evaluation as part of the tuning job.</span>
<span class="sd">        evaluation_interval: The evaluation will run at every</span>
<span class="sd">            evaluation_interval tuning steps. Default: 20.</span>
<span class="sd">        enable_early_stopping: If True, the tuning may stop early before</span>
<span class="sd">            completing all the tuning steps. Requires evaluation_data.</span>
<span class="sd">        enable_checkpoint_selection: If set to True, the tuning process returns</span>
<span class="sd">            the best model checkpoint (based on model evaluation).</span>
<span class="sd">            If set to False, the latest model checkpoint is returned.</span>
<span class="sd">            If unset, the selection is only enabled for `*-bison@001` models.</span>
<span class="sd">        tensorboard: Vertex Tensorboard where to write the evaluation metrics.</span>
<span class="sd">            The Tensorboard must be in the same location as the tuning job.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">evaluation_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">evaluation_interval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">enable_early_stopping</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">enable_checkpoint_selection</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">tensorboard</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">aiplatform</span><span class="o">.</span><span class="n">Tensorboard</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="c1"># Evaluation spec fields that are not supported by RLHF tuning</span>
<span class="n">_UNUSED_RLHF_EVAL_SPECS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">,</span>
    <span class="s2">&quot;enable_early_stopping&quot;</span><span class="p">,</span>
    <span class="s2">&quot;enable_checkpoint_selection&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_unused_rlhf_eval_spec_error_msg</span><span class="p">(</span>
    <span class="n">unused_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the user-facing error message if an unused evaluation fields was set.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">unused_key</span><span class="si">}</span><span class="s2"> is not supported by RLHF tuning. &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Please leave </span><span class="si">{</span><span class="n">unused_key</span><span class="si">}</span><span class="s2"> as None.&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_unused_rlhf_eval_specs</span><span class="p">(</span><span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">TuningEvaluationSpec</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Raises an error if any unused evaluation fields are specified for RLHF tuning job.&quot;&quot;&quot;</span>
    <span class="n">eval_spec_dict</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">asdict</span><span class="p">(</span><span class="n">tuning_evaluation_spec</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">unused_key</span> <span class="ow">in</span> <span class="n">_UNUSED_RLHF_EVAL_SPECS</span><span class="p">:</span>
        <span class="n">unused_value</span> <span class="o">=</span> <span class="n">eval_spec_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">unused_key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unused_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">_get_unused_rlhf_eval_spec_error_msg</span><span class="p">(</span><span class="n">unused_key</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">_GroundingSourceBase</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Interface of grounding source dataclass for grounding.&quot;&quot;&quot;</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_to_grounding_source_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;construct grounding source into dictionary&quot;&quot;&quot;</span>
        <span class="k">pass</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">WebSearch</span><span class="p">(</span><span class="n">_GroundingSourceBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;WebSearch represents a grounding source using public web search.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        disable_attribution: If set to `True`, skip finding claim attributions (i.e not generate grounding citation). Default: False.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">disable_attribution</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;WEB&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="nb">repr</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_to_grounding_source_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;sources&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_type</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">],</span>
            <span class="s2">&quot;disableAttribution&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_attribution</span><span class="p">,</span>
        <span class="p">}</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">InlineContext</span><span class="p">(</span><span class="n">_GroundingSourceBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;InlineContext represents a grounding source using provided inline context.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        inline_context: The content used as inline context.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">inline_context</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;INLINE&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="nb">repr</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_to_grounding_source_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;sources&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_type</span><span class="p">,</span>
                    <span class="s2">&quot;inlineContext&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inline_context</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">],</span>
        <span class="p">}</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">VertexAISearch</span><span class="p">(</span><span class="n">_GroundingSourceBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VertexAISearchDatastore represents a grounding source using Vertex AI Search datastore</span>
<span class="sd">    Attributes:</span>
<span class="sd">        data_store_id: Data store ID of the Vertex AI Search datastore.</span>
<span class="sd">        location: GCP multi region where you have set up your Vertex AI Search data store. Possible values can be `global`, `us`, `eu`, etc.</span>
<span class="sd">            Learn more about Vertex AI Search location here:</span>
<span class="sd">            https://cloud.google.com/generative-ai-app-builder/docs/locations</span>
<span class="sd">        project: The project where you have set up your Vertex AI Search.</span>
<span class="sd">            If not specified, will assume that your Vertex AI Search is within your current project.</span>
<span class="sd">        disable_attribution: If set to `True`, skip finding claim attributions (i.e not generate grounding citation). Default: False.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">data_store_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">location</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">project</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">disable_attribution</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;VERTEX_AI_SEARCH&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="nb">repr</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_datastore_path</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">_project</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project</span> <span class="ow">or</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">project</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;projects/</span><span class="si">{</span><span class="n">_project</span><span class="si">}</span><span class="s2">/locations/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">location</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;/collections/default_collection/dataStores/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data_store_id</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_to_grounding_source_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;sources&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_type</span><span class="p">,</span>
                    <span class="s2">&quot;vertexAiSearchDatastore&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_datastore_path</span><span class="p">(),</span>
                <span class="p">}</span>
            <span class="p">],</span>
            <span class="s2">&quot;disableAttribution&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_attribution</span><span class="p">,</span>
        <span class="p">}</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">GroundingSource</span><span class="p">:</span>

    <span class="n">WebSearch</span> <span class="o">=</span> <span class="n">WebSearch</span>
    <span class="n">VertexAISearch</span> <span class="o">=</span> <span class="n">VertexAISearch</span>
    <span class="n">InlineContext</span> <span class="o">=</span> <span class="n">InlineContext</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">GroundingCitation</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Citaion used from grounding.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        start_index: Index in the prediction output where the citation starts</span>
<span class="sd">            (inclusive). Must be &gt;= 0 and &lt; end_index.</span>
<span class="sd">        end_index: Index in the prediction output where the citation ends</span>
<span class="sd">            (exclusive). Must be &gt; start_index and &lt; len(output).</span>
<span class="sd">        url: URL associated with this citation. If present, this URL links to the</span>
<span class="sd">            webpage of the source of this citation. Possible URLs include news</span>
<span class="sd">            websites, GitHub repos, etc.</span>
<span class="sd">        title: Title associated with this citation. If present, it refers to the title</span>
<span class="sd">            of the source of this citation. Possible titles include</span>
<span class="sd">            news titles, book titles, etc.</span>
<span class="sd">        license: License associated with this citation. If present, it refers to the</span>
<span class="sd">            license of the source of this citation. Possible licenses include code</span>
<span class="sd">            licenses, e.g., mit license.</span>
<span class="sd">        publication_date: Publication date associated with this citation. If present, it refers to</span>
<span class="sd">            the date at which the source of this citation was published.</span>
<span class="sd">            Possible formats are YYYY, YYYY-MM, YYYY-MM-DD.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">start_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">end_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">url</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">license</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">publication_date</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">GroundingMetadata</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Metadata for grounding.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        citations: List of grounding citations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">citations</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">GroundingCitation</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">search_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_parse_citation_from_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">citation_dict_camel</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GroundingCitation</span><span class="p">:</span>
        <span class="n">_start_index</span> <span class="o">=</span> <span class="n">citation_dict_camel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;startIndex&quot;</span><span class="p">)</span>
        <span class="n">_end_index</span> <span class="o">=</span> <span class="n">citation_dict_camel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;endIndex&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_start_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_start_index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">_start_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_end_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_end_index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">_end_index</span><span class="p">)</span>
        <span class="n">_url</span> <span class="o">=</span> <span class="n">citation_dict_camel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;url&quot;</span><span class="p">)</span>
        <span class="n">_title</span> <span class="o">=</span> <span class="n">citation_dict_camel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;title&quot;</span><span class="p">)</span>
        <span class="n">_license</span> <span class="o">=</span> <span class="n">citation_dict_camel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;license&quot;</span><span class="p">)</span>
        <span class="n">_publication_date</span> <span class="o">=</span> <span class="n">citation_dict_camel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;publicationDate&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">GroundingCitation</span><span class="p">(</span>
            <span class="n">start_index</span><span class="o">=</span><span class="n">_start_index</span><span class="p">,</span>
            <span class="n">end_index</span><span class="o">=</span><span class="n">_end_index</span><span class="p">,</span>
            <span class="n">url</span><span class="o">=</span><span class="n">_url</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="n">_title</span><span class="p">,</span>
            <span class="n">license</span><span class="o">=</span><span class="n">_license</span><span class="p">,</span>
            <span class="n">publication_date</span><span class="o">=</span><span class="n">_publication_date</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">citations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_parse_citation_from_dict</span><span class="p">(</span><span class="n">citation</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">citation</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;citations&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">search_queries</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;searchQueries&quot;</span><span class="p">,</span> <span class="p">[])</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextGenerationResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TextGenerationResponse represents a response of a language model.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        text: The generated text</span>
<span class="sd">        is_blocked: Whether the the request was blocked.</span>
<span class="sd">        errors: The error codes indicate why the response was blocked.</span>
<span class="sd">            Learn more information about safety errors here:</span>
<span class="sd">            this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors</span>
<span class="sd">        safety_attributes: Scores for safety attributes.</span>
<span class="sd">            Learn more about the safety attributes here:</span>
<span class="sd">            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</span>
<span class="sd">        grounding_metadata: Metadata for grounding.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">is_blocked</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">errors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
    <span class="n">safety_attributes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
    <span class="n">grounding_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GroundingMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span>
        <span class="c1"># Falling back to the full representation</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">grounding_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="s2">&quot;TextGenerationResponse(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;text=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, is_blocked=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_blocked</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, errors=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, safety_attributes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_attributes</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, grounding_metadata=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grounding_metadata</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="s2">&quot;TextGenerationResponse(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;text=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, is_blocked=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_blocked</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, errors=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, safety_attributes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_attributes</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">raw_prediction_response</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Raw prediction response.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prediction_response</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">MultiCandidateTextGenerationResponse</span><span class="p">(</span><span class="n">TextGenerationResponse</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents a multi-candidate response of a language model.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        text: The generated text for the first candidate.</span>
<span class="sd">        is_blocked: Whether the first candidate response was blocked.</span>
<span class="sd">        safety_attributes: Scores for safety attributes for the first candidate.</span>
<span class="sd">            Learn more about the safety attributes here:</span>
<span class="sd">            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</span>
<span class="sd">        grounding_metadata: Grounding metadata for the first candidate.</span>
<span class="sd">        candidates: The candidate responses.</span>
<span class="sd">            Usually contains a single candidate unless `candidate_count` is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_repr_pretty_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">cycle</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pretty prints self in IPython environments.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">cycle</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(...)&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">candidates</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">_TextGenerationModel</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TextGenerationModel represents a general language model.</span>

<span class="sd">    Examples::</span>

<span class="sd">        # Getting answers:</span>
<span class="sd">        model = TextGenerationModel.from_pretrained(&quot;text-bison@001&quot;)</span>
<span class="sd">        model.predict(&quot;What is life?&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/text_generation_1.0.0.yaml&quot;</span>

    <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span> <span class="o">=</span> <span class="mi">128</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets model response for a single prompt.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Question to ask the model.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of response candidates to return.</span>
<span class="sd">            grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>
<span class="sd">            logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">                at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">                returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">                The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">                probabilities are returned.</span>
<span class="sd">                The maximum value for `logprobs` is 5.</span>
<span class="sd">            presence_penalty:</span>
<span class="sd">                Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">                thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            frequency_penalty:</span>
<span class="sd">                Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">                text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            logit_bias:</span>
<span class="sd">                Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">                The bias values are added to the logits before sampling.</span>
<span class="sd">                Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">                Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">                Range: [-100.0, 100.0]</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
            <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
            <span class="n">prediction_response</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously gets model response for a single prompt.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Question to ask the model.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of response candidates to return.</span>
<span class="sd">            grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>
<span class="sd">            logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">                at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">                returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">                The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">                probabilities are returned.</span>
<span class="sd">                The maximum value for `logprobs` is 5.</span>
<span class="sd">            presence_penalty:</span>
<span class="sd">                Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">                thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            frequency_penalty:</span>
<span class="sd">                Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">                text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            logit_bias:</span>
<span class="sd">                Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">                The bias values are added to the logits before sampling.</span>
<span class="sd">                Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">                Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">                Range: [-100.0, 100.0]</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
            <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
            <span class="n">prediction_response</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets a streaming model response for a single prompt.</span>

<span class="sd">        The result is a stream (generator) of partial responses.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Question to ask the model.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">                at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">                returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">                The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">                probabilities are returned.</span>
<span class="sd">                The maximum value for `logprobs` is 5.</span>
<span class="sd">            presence_penalty:</span>
<span class="sd">                Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">                thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            frequency_penalty:</span>
<span class="sd">                Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">                text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            logit_bias:</span>
<span class="sd">                Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">                The bias values are added to the logits before sampling.</span>
<span class="sd">                Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">                Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">                Range: [-100.0, 100.0]</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_service_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span>
        <span class="k">for</span> <span class="p">(</span>
            <span class="n">prediction_dict</span>
        <span class="p">)</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict</span><span class="p">(</span>
            <span class="n">prediction_service_client</span><span class="o">=</span><span class="n">prediction_service_client</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">prediction_obj</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
                <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
                <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">yield</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span><span class="n">prediction_obj</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously gets a streaming model response for a single prompt.</span>

<span class="sd">        The result is a stream (generator) of partial responses.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Question to ask the model.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">                at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">                returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">                The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">                probabilities are returned.</span>
<span class="sd">                The maximum value for `logprobs` is 5.</span>
<span class="sd">            presence_penalty:</span>
<span class="sd">                Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">                thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            frequency_penalty:</span>
<span class="sd">                Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">                text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">                Range: [-2.0, 2.0]</span>
<span class="sd">            logit_bias:</span>
<span class="sd">                Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">                The bias values are added to the logits before sampling.</span>
<span class="sd">                Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">                Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">                Range: [-100.0, 100.0]</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_service_async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_async_client</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">prediction_dict</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict_async</span><span class="p">(</span>
            <span class="n">prediction_service_async_client</span><span class="o">=</span><span class="n">prediction_service_async_client</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">prediction_obj</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
                <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
                <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">yield</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span><span class="n">prediction_obj</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_text_generation_prediction_request</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_PredictionRequest&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prepares the text generation request for a single prompt.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt: Question to ask the model.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>
<span class="sd">        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>
<span class="sd">        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">            at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">            probabilities are returned.</span>
<span class="sd">            The maximum value for `logprobs` is 5.</span>
<span class="sd">        presence_penalty:</span>
<span class="sd">            Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">            thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        frequency_penalty:</span>
<span class="sd">            Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">            text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        logit_bias:</span>
<span class="sd">            Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">            The bias values are added to the logits before sampling.</span>
<span class="sd">            Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">            Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">            Range: [-100.0, 100.0]</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `_PredictionRequest` object that contains prediction instance and parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">instance</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
    <span class="n">prediction_parameters</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">max_output_tokens</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;maxDecodeSteps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_output_tokens</span>

    <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;temperature&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">temperature</span>

    <span class="k">if</span> <span class="n">top_p</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_p</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">top_p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">top_p</span><span class="p">)</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;topP&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_p</span>

    <span class="k">if</span> <span class="n">top_k</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;topK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_k</span>

    <span class="k">if</span> <span class="n">stop_sequences</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;stopSequences&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stop_sequences</span>

    <span class="k">if</span> <span class="n">candidate_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;candidateCount&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate_count</span>

    <span class="k">if</span> <span class="n">grounding_source</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span>
            <span class="s2">&quot;groundingConfig&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">grounding_source</span><span class="o">.</span><span class="n">_to_grounding_source_dict</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">logprobs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;logprobs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logprobs</span>

    <span class="k">if</span> <span class="n">presence_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;presencePenalty&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">presence_penalty</span>

    <span class="k">if</span> <span class="n">frequency_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;frequencyPenalty&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency_penalty</span>

    <span class="k">if</span> <span class="n">logit_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;logitBias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">logit_bias</span>

    <span class="k">return</span> <span class="n">_PredictionRequest</span><span class="p">(</span>
        <span class="n">instance</span><span class="o">=</span><span class="n">instance</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_parameters</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_parse_text_generation_model_response</span><span class="p">(</span>
    <span class="n">prediction_response</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span>
    <span class="n">prediction_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TextGenerationResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts the raw text_generation model response to `TextGenerationResponse`.&quot;&quot;&quot;</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">[</span><span class="n">prediction_idx</span><span class="p">]</span>
    <span class="n">safety_attributes_dict</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;safetyAttributes&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">grounding_metadata_dict</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;groundingMetadata&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">errors_list</span> <span class="o">=</span> <span class="n">safety_attributes_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;errors&quot;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">errors_list</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">TextGenerationResponse</span><span class="p">(</span>
        <span class="n">text</span><span class="o">=</span><span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">],</span>
        <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
        <span class="n">is_blocked</span><span class="o">=</span><span class="n">safety_attributes_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
        <span class="n">errors</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span>
        <span class="n">safety_attributes</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="n">safety_attributes_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;categories&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">[],</span>
                <span class="n">safety_attributes_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">[],</span>
            <span class="p">)</span>
        <span class="p">),</span>
        <span class="n">grounding_metadata</span><span class="o">=</span><span class="n">GroundingMetadata</span><span class="p">(</span><span class="n">grounding_metadata_dict</span><span class="p">),</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
    <span class="n">prediction_response</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiCandidateTextGenerationResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts the raw text_generation model response to `MultiCandidateTextGenerationResponse`.&quot;&quot;&quot;</span>
    <span class="c1"># The contract for the PredictionService is that there is a 1:1 mapping</span>
    <span class="c1"># between request `instances` and response `predictions`.</span>
    <span class="c1"># Unfortunetely, the text-bison models violate this contract.</span>

    <span class="n">prediction_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">prediction_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">prediction_count</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
            <span class="n">prediction_idx</span><span class="o">=</span><span class="n">prediction_idx</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">MultiCandidateTextGenerationResponse</span><span class="p">(</span>
        <span class="n">text</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
        <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
        <span class="n">is_blocked</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_blocked</span><span class="p">,</span>
        <span class="n">errors</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">errors</span><span class="p">,</span>
        <span class="n">safety_attributes</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">safety_attributes</span><span class="p">,</span>
        <span class="n">grounding_metadata</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grounding_metadata</span><span class="p">,</span>
        <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">_ModelWithBatchPredict</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model that supports batch prediction.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">batch_predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">destination_uri_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">model_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">BatchPredictionJob</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a batch prediction job with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset: The location of the dataset.</span>
<span class="sd">                `gs://` and `bq://` URIs are supported.</span>
<span class="sd">            destination_uri_prefix: The URI prefix for the prediction.</span>
<span class="sd">                `gs://` and `bq://` URIs are supported.</span>
<span class="sd">            model_parameters: Model-specific parameters to send to the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `BatchPredictionJob` object</span>
<span class="sd">        Raises:</span>
<span class="sd">            ValueError: When source or destination URI is not supported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">arguments</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">first_source_uri</span> <span class="o">=</span> <span class="n">dataset</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">first_source_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">uri</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;All URIs in the list must start with &#39;gs://&#39;: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;gcs_source&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="k">elif</span> <span class="n">first_source_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;bq://&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Only single BigQuery source can be specified: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;bigquery_source&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported source_uri: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">destination_uri_prefix</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
            <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;gcs_destination_prefix&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">destination_uri_prefix</span>
        <span class="k">elif</span> <span class="n">destination_uri_prefix</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;bq://&quot;</span><span class="p">):</span>
            <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;bigquery_destination_prefix&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">destination_uri_prefix</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported destination_uri: </span><span class="si">{</span><span class="n">destination_uri_prefix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">model_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_resource_name</span>

        <span class="n">job</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">BatchPredictionJob</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">job_display_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="o">**</span><span class="n">arguments</span><span class="p">,</span>
            <span class="n">model_parameters</span><span class="o">=</span><span class="n">model_parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">job</span>


<span class="k">class</span> <span class="nc">_PreviewModelWithBatchPredict</span><span class="p">(</span><span class="n">_ModelWithBatchPredict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model that supports batch prediction.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">batch_predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">destination_uri_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">BatchPredictionJob</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a batch prediction job with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset: Required. The location of the dataset.</span>
<span class="sd">                `gs://` and `bq://` URIs are supported.</span>
<span class="sd">            destination_uri_prefix: The URI prefix for the prediction.</span>
<span class="sd">                `gs://` and `bq://` URIs are supported.</span>
<span class="sd">            model_parameters: Model-specific parameters to send to the model.</span>
<span class="sd">            **_kwargs: Deprecated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `BatchPredictionJob` object</span>
<span class="sd">        Raises:</span>
<span class="sd">            ValueError: When source or destination URI is not supported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;source_uri&quot;</span> <span class="ow">in</span> <span class="n">_kwargs</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;source_uri is deprecated, use dataset instead.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">dataset</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;source_uri is deprecated, use dataset instead.&quot;</span><span class="p">)</span>
            <span class="n">dataset</span> <span class="o">=</span> <span class="n">_kwargs</span><span class="p">[</span><span class="s2">&quot;source_uri&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dataset</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dataset must be specified&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">batch_predict</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">destination_uri_prefix</span><span class="o">=</span><span class="n">destination_uri_prefix</span><span class="p">,</span>
            <span class="n">model_parameters</span><span class="o">=</span><span class="n">model_parameters</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">TextGenerationModel</span><span class="p">(</span>
    <span class="n">_TextGenerationModel</span><span class="p">,</span>
    <span class="n">_TunableTextModelMixin</span><span class="p">,</span>
    <span class="n">_ModelWithBatchPredict</span><span class="p">,</span>
    <span class="n">_RlhfTunableModelMixin</span><span class="p">,</span>
<span class="p">):</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>


<span class="k">class</span> <span class="nc">_ChatModel</span><span class="p">(</span><span class="n">_TextGenerationModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatModel represents a language model that is capable of chat.</span>

<span class="sd">    Examples::</span>

<span class="sd">        # Getting answers:</span>
<span class="sd">        model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)</span>
<span class="sd">        model.predict(&quot;What is life?&quot;)</span>

<span class="sd">        # Chat:</span>
<span class="sd">        chat = model.start_chat()</span>

<span class="sd">        chat.send_message(&quot;Do you know any cool events this weekend?&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_ChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_ChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_ChatSession</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatSession represents a chat session with a language model.</span>

<span class="sd">    Within a chat session, the model keeps context and remembers the previous conversation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">_ChatModel</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_output_tokens</span> <span class="o">=</span> <span class="n">max_output_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_top_k</span> <span class="o">=</span> <span class="n">top_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_top_p</span> <span class="o">=</span> <span class="n">top_p</span>

    <span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `TextGenerationResponse` object that contains the text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">new_history_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_history_text</span><span class="p">:</span>
            <span class="n">new_history_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_history_text</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="n">new_history_text</span> <span class="o">+=</span> <span class="n">message</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="n">response_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">new_history_text</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span>
            <span class="k">if</span> <span class="n">max_output_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span> <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span> <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span> <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_top_p</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">response_text</span> <span class="o">=</span> <span class="n">response_obj</span><span class="o">.</span><span class="n">text</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">message</span><span class="p">,</span> <span class="n">response_text</span><span class="p">))</span>
        <span class="n">new_history_text</span> <span class="o">+=</span> <span class="n">response_text</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history_text</span> <span class="o">=</span> <span class="n">new_history_text</span>
        <span class="k">return</span> <span class="n">response_obj</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbeddingInput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Structural text embedding input.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        text: The main text content to embed.</span>
<span class="sd">        task_type: The name of the downstream task the embeddings will be used for.</span>
<span class="sd">            Valid values:</span>
<span class="sd">            RETRIEVAL_QUERY</span>
<span class="sd">                Specifies the given text is a query in a search/retrieval setting.</span>
<span class="sd">            RETRIEVAL_DOCUMENT</span>
<span class="sd">                Specifies the given text is a document from the corpus being searched.</span>
<span class="sd">            SEMANTIC_SIMILARITY</span>
<span class="sd">                Specifies the given text will be used for STS.</span>
<span class="sd">            CLASSIFICATION</span>
<span class="sd">                Specifies that the given text will be classified.</span>
<span class="sd">            CLUSTERING</span>
<span class="sd">                Specifies that the embeddings will be used for clustering.</span>
<span class="sd">            QUESTION_ANSWERING</span>
<span class="sd">                Specifies that the embeddings will be used for question answering.</span>
<span class="sd">            FACT_VERIFICATION</span>
<span class="sd">                Specifies that the embeddings will be used for fact verification.</span>
<span class="sd">        title: Optional identifier of the text content.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">_TextEmbeddingModel</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TextEmbeddingModel class calculates embeddings for the given texts.</span>

<span class="sd">    Examples::</span>

<span class="sd">        # Getting embedding:</span>
<span class="sd">        model = TextEmbeddingModel.from_pretrained(&quot;textembedding-gecko@001&quot;)</span>
<span class="sd">        embeddings = model.get_embeddings([&quot;What is life?&quot;])</span>
<span class="sd">        for embedding in embeddings:</span>
<span class="sd">            vector = embedding.values</span>
<span class="sd">            print(len(vector))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/text_embedding_1.0.0.yaml&quot;</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_text_embedding_request</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TextEmbeddingInput</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_MultiInstancePredictionRequest</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously calculates embeddings for the given texts.</span>

<span class="sd">        Args:</span>
<span class="sd">            texts(str): A list of texts or `TextEmbeddingInput` objects to embed.</span>
<span class="sd">            auto_truncate(bool): Whether to automatically truncate long texts. Default: True.</span>
<span class="sd">            output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `_MultiInstancePredictionRequest` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The `texts` argument must be a list, not a single string.&quot;</span><span class="p">)</span>
        <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">TextEmbeddingInput</span><span class="p">):</span>
                <span class="n">instance</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text</span><span class="o">.</span><span class="n">text</span><span class="p">}</span>
                <span class="k">if</span> <span class="n">text</span><span class="o">.</span><span class="n">task_type</span><span class="p">:</span>
                    <span class="n">instance</span><span class="p">[</span><span class="s2">&quot;task_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">task_type</span>
                <span class="k">if</span> <span class="n">text</span><span class="o">.</span><span class="n">title</span><span class="p">:</span>
                    <span class="n">instance</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">title</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">instance</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported text embedding input type: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;autoTruncate&quot;</span><span class="p">:</span> <span class="n">auto_truncate</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">output_dimensionality</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;outputDimensionality&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_dimensionality</span>
        <span class="k">return</span> <span class="n">_MultiInstancePredictionRequest</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="n">instances</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TextEmbeddingInput</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;TextEmbedding&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculates embeddings for the given texts.</span>

<span class="sd">        Args:</span>
<span class="sd">            texts: A list of texts or `TextEmbeddingInput` objects to embed.</span>
<span class="sd">            auto_truncate: Whether to automatically truncate long texts. Default: True.</span>
<span class="sd">            output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of `TextEmbedding` objects.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_text_embedding_request</span><span class="p">(</span>
            <span class="n">texts</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
            <span class="n">auto_truncate</span><span class="o">=</span><span class="n">auto_truncate</span><span class="p">,</span>
            <span class="n">output_dimensionality</span><span class="o">=</span><span class="n">output_dimensionality</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instances</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">TextEmbedding</span><span class="o">.</span><span class="n">_parse_text_embedding_response</span><span class="p">(</span>
                <span class="n">prediction_response</span><span class="p">,</span> <span class="n">i_prediction</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i_prediction</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">get_embeddings_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TextEmbeddingInput</span><span class="p">]],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;TextEmbedding&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously calculates embeddings for the given texts.</span>

<span class="sd">        Args:</span>
<span class="sd">            texts: A list of texts or `TextEmbeddingInput` objects to embed.</span>
<span class="sd">            auto_truncate: Whether to automatically truncate long texts. Default: True.</span>
<span class="sd">            output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of `TextEmbedding` objects.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_text_embedding_request</span><span class="p">(</span>
            <span class="n">texts</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
            <span class="n">auto_truncate</span><span class="o">=</span><span class="n">auto_truncate</span><span class="p">,</span>
            <span class="n">output_dimensionality</span><span class="o">=</span><span class="n">output_dimensionality</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instances</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">TextEmbedding</span><span class="o">.</span><span class="n">_parse_text_embedding_response</span><span class="p">(</span>
                <span class="n">prediction_response</span><span class="p">,</span> <span class="n">i_prediction</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i_prediction</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
        <span class="p">]</span>


<span class="c1"># TODO(b/625884109): Support Union[str, &quot;pandas.core.frame.DataFrame&quot;]</span>
<span class="c1"># for corpus, queries, test and validation data.</span>
<span class="c1"># TODO(b/625884109): Validate input args, batch_size &gt;0 and train_steps &gt;30, and</span>
<span class="c1"># task_type must be &#39;DEFAULT&#39; or None if _model_id is textembedding-gecko@001.</span>
<span class="k">class</span> <span class="nc">_TunableTextEmbeddingModelMixin</span><span class="p">(</span><span class="n">_TunableModelMixin</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>  <span class="c1"># Unused.</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Use deploy_tuned_model instead to get the tuned model.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">training_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">corpus_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">queries_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">test_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">machine_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">        This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">        Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        tuning_job = model.tune_model(...)</span>
<span class="sd">        ... do some other work</span>
<span class="sd">        tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>

<span class="sd">        Args:</span>
<span class="sd">            training_data: URI pointing to training data in TSV format.</span>
<span class="sd">            corpus_data: URI pointing to data in JSON lines format.</span>
<span class="sd">            queries_data: URI pointing to data in JSON lines format.</span>
<span class="sd">            test_data: URI pointing to data in TSV format.</span>
<span class="sd">            validation_data: URI pointing to data in TSV format.</span>
<span class="sd">            batch_size: Size of batch.</span>
<span class="sd">            train_steps: Number of training batches to tune on.</span>
<span class="sd">            tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">            model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            task_type: Type of task. Can be &quot;RETRIEVAL_QUERY&quot;, &quot;RETRIEVAL_DOCUMENT&quot;, &quot;SEMANTIC_SIMILARITY&quot;, &quot;CLASSIFICATION&quot;, &quot;CLUSTERING&quot;, &quot;QUESTION_ANSWERING&quot;, or &quot;FACT_VERIFICATION&quot;.</span>
<span class="sd">            machine_type: Machine type. E.g., &quot;a2-highgpu-1g&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator_count: Count of accelerators.</span>
<span class="sd">            accelerator: Kind of accelerator. E.g., &quot;NVIDIA_TESLA_A100&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">            Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">            RuntimeError: If the model does not support tuning</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
            <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
            <span class="n">corpus_data</span><span class="o">=</span><span class="n">corpus_data</span><span class="p">,</span>
            <span class="n">queries_data</span><span class="o">=</span><span class="n">queries_data</span><span class="p">,</span>
            <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
            <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_data</span><span class="p">,</span>
            <span class="n">task_type</span><span class="o">=</span><span class="n">task_type</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
            <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
            <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
            <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
            <span class="n">accelerator</span><span class="o">=</span><span class="n">accelerator</span><span class="p">,</span>
            <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_bundle_up_tuning_job</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pipeline_job</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_TextEmbeddingModelTuningJob</span><span class="p">(</span>
            <span class="n">base_model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">job</span><span class="o">=</span><span class="n">pipeline_job</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">deploy_tuned_model</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">tuned_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">machine_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the specified tuned language model.</span>

<span class="sd">        Args:</span>
<span class="sd">            machine_type: Machine type. E.g., &quot;a2-highgpu-1g&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator: Kind of accelerator. E.g., &quot;NVIDIA_TESLA_A100&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator_count: Count of accelerators.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuned `LanguageModel` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tuned_vertex_model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">)</span>
        <span class="n">tuned_model_labels</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span>

        <span class="k">if</span> <span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_model_labels</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The provided model </span><span class="si">{</span><span class="n">tuned_model_name</span><span class="si">}</span><span class="s2"> does not have a base model ID.&quot;</span>
            <span class="p">)</span>

        <span class="n">tuning_model_id</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="p">]</span>
        <span class="n">tuned_model_deployments</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">deployed_models</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tuned_model_deployments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Deploying a model to an endpoint requires a resource quota.</span>
            <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
                <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
                <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator</span><span class="p">,</span>
                <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">resource_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model_deployments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">endpoint</span>

        <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">)</span>
        <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
            <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="bp">cls</span><span class="p">},</span>
        <span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">interface_class</span><span class="p">(</span>
            <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">model</span>


<span class="k">class</span> <span class="nc">TextEmbeddingModel</span><span class="p">(</span><span class="n">_TextEmbeddingModel</span><span class="p">,</span> <span class="n">_TunableTextEmbeddingModelMixin</span><span class="p">):</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>


<span class="k">class</span> <span class="nc">_PreviewTextEmbeddingModel</span><span class="p">(</span>
    <span class="n">TextEmbeddingModel</span><span class="p">,</span>
    <span class="n">_ModelWithBatchPredict</span><span class="p">,</span>
    <span class="n">_CountTokensMixin</span><span class="p">,</span>
<span class="p">):</span>
    <span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;TextEmbeddingModel&quot;</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbeddingStatistics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Text embedding statistics.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">token_count</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">truncated</span><span class="p">:</span> <span class="nb">bool</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Text embedding vector and statistics.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextEmbeddingStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_parse_text_embedding_response</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">prediction_response</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span> <span class="n">prediction_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextEmbedding&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a `TextEmbedding` object from a prediction.</span>

<span class="sd">        Args:</span>
<span class="sd">            prediction_response: `aiplatform.models.Prediction` object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `TextEmbedding` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">[</span><span class="n">prediction_index</span><span class="p">]</span>
        <span class="n">is_prediction_from_pretrained_models</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">prediction</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Mapping</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_prediction_from_pretrained_models</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;embeddings&quot;</span><span class="p">]</span>
            <span class="n">embedding_stats</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;statistics&quot;</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
                <span class="n">values</span><span class="o">=</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">],</span>
                <span class="n">statistics</span><span class="o">=</span><span class="n">TextEmbeddingStatistics</span><span class="p">(</span>
                    <span class="n">token_count</span><span class="o">=</span><span class="n">embedding_stats</span><span class="p">[</span><span class="s2">&quot;token_count&quot;</span><span class="p">],</span>
                    <span class="n">truncated</span><span class="o">=</span><span class="n">embedding_stats</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">],</span>
                <span class="p">),</span>
                <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">)</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">InputOutputTextPair</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;InputOutputTextPair represents a pair of input and output texts.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">output_text</span><span class="p">:</span> <span class="nb">str</span>


<span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ChatMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A chat message.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        content: Content of the message.</span>
<span class="sd">        author: Author of the message.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">content</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">author</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span> <span class="nc">_ChatModelBase</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;_ChatModelBase is a base class for chat models.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">                For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style</span>
<span class="sd">            examples: List of structured messages to the model to learn how to respond to the conversation.</span>
<span class="sd">                A list of `InputOutputTextPair` objects.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            message_history: A list of previously sent and received messages.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">ChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">ChatModel</span><span class="p">(</span><span class="n">_ChatModelBase</span><span class="p">,</span> <span class="n">_TunableChatModelMixin</span><span class="p">,</span> <span class="n">_RlhfTunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatModel represents a language model that is capable of chat.</span>

<span class="sd">    Examples::</span>

<span class="sd">        chat_model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)</span>

<span class="sd">        chat = chat_model.start_chat(</span>
<span class="sd">            context=&quot;My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.&quot;,</span>
<span class="sd">            examples=[</span>
<span class="sd">                InputOutputTextPair(</span>
<span class="sd">                    input_text=&quot;Who do you work for?&quot;,</span>
<span class="sd">                    output_text=&quot;I work for Ned.&quot;,</span>
<span class="sd">                ),</span>
<span class="sd">                InputOutputTextPair(</span>
<span class="sd">                    input_text=&quot;What do I like?&quot;,</span>
<span class="sd">                    output_text=&quot;Ned likes watching movies.&quot;,</span>
<span class="sd">                ),</span>
<span class="sd">            ],</span>
<span class="sd">            temperature=0.3,</span>
<span class="sd">        )</span>

<span class="sd">        chat.send_message(&quot;Do you know any cool events this weekend?&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/chat_generation_1.0.0.yaml&quot;</span>


<span class="k">class</span> <span class="nc">_PreviewChatModel</span><span class="p">(</span><span class="n">ChatModel</span><span class="p">,</span> <span class="n">_PreviewTunableChatModelMixin</span><span class="p">):</span>
    <span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;ChatModel&quot;</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>

    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_PreviewChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">                For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style</span>
<span class="sd">            examples: List of structured messages to the model to learn how to respond to the conversation.</span>
<span class="sd">                A list of `InputOutputTextPair` objects.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            message_history: A list of previously sent and received messages.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_PreviewChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">CodeChatModel</span><span class="p">(</span><span class="n">_ChatModelBase</span><span class="p">,</span> <span class="n">_TunableChatModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CodeChatModel represents a model that is capable of completing code.</span>

<span class="sd">    Examples:</span>
<span class="sd">        code_chat_model = CodeChatModel.from_pretrained(&quot;codechat-bison@001&quot;)</span>

<span class="sd">        code_chat = code_chat_model.start_chat(</span>
<span class="sd">            context=&quot;I&#39;m writing a large-scale enterprise application.&quot;,</span>
<span class="sd">            max_output_tokens=128,</span>
<span class="sd">            temperature=0.2,</span>
<span class="sd">        )</span>

<span class="sd">        code_chat.send_message(&quot;Please help write a function to calculate the min of two numbers&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/codechat_generation_1.0.0.yaml&quot;</span>

<div class="viewcode-block" id="CodeChatModel.start_chat"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models.CodeChatModel.start_chat">[docs]</a>    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;CodeChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the code chat model.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">                For example, you can use context to specify words the model can or</span>
<span class="sd">                cannot use, topics to focus on or avoid, or the response format or style.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">CodeChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_PreviewCodeChatModel</span><span class="p">(</span><span class="n">CodeChatModel</span><span class="p">):</span>
    <span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;CodeChatModel&quot;</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>

    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_PreviewCodeChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">                For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style</span>
<span class="sd">            examples: List of structured messages to the model to learn how to respond to the conversation.</span>
<span class="sd">                A list of `InputOutputTextPair` objects.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            message_history: A list of previously sent and received messages.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_PreviewCodeChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_ChatSessionBase</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;_ChatSessionBase is a base class for all chat sessions.&quot;&quot;&quot;</span>

    <span class="n">USER_AUTHOR</span> <span class="o">=</span> <span class="s2">&quot;user&quot;</span>
    <span class="n">MODEL_AUTHOR</span> <span class="o">=</span> <span class="s2">&quot;bot&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">_ChatModelBase</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="n">context</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_examples</span> <span class="o">=</span> <span class="n">examples</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_output_tokens</span> <span class="o">=</span> <span class="n">max_output_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_top_k</span> <span class="o">=</span> <span class="n">top_k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_top_p</span> <span class="o">=</span> <span class="n">top_p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]</span> <span class="o">=</span> <span class="n">message_history</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stop_sequences</span> <span class="o">=</span> <span class="n">stop_sequences</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">message_history</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of previous messages.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span>

    <span class="k">def</span> <span class="nf">_prepare_request</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PredictionRequest</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prepares a request for the language model.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>
<span class="sd">            grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `_PredictionRequest` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_parameters</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">max_output_tokens</span> <span class="o">=</span> <span class="n">max_output_tokens</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_output_tokens</span>
        <span class="k">if</span> <span class="n">max_output_tokens</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;maxDecodeSteps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_output_tokens</span>

        <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">temperature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_temperature</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;temperature&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">temperature</span>

        <span class="n">top_p</span> <span class="o">=</span> <span class="n">top_p</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_top_p</span>
        <span class="k">if</span> <span class="n">top_p</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">top_p</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">top_p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">top_p</span><span class="p">)</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;topP&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_p</span>

        <span class="n">top_k</span> <span class="o">=</span> <span class="n">top_k</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_top_k</span>
        <span class="k">if</span> <span class="n">top_k</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;topK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_k</span>

        <span class="n">stop_sequences</span> <span class="o">=</span> <span class="n">stop_sequences</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_sequences</span>
        <span class="k">if</span> <span class="n">stop_sequences</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;stopSequences&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stop_sequences</span>

        <span class="k">if</span> <span class="n">candidate_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;candidateCount&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate_count</span>

        <span class="k">if</span> <span class="n">grounding_source</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span>
                <span class="s2">&quot;groundingConfig&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">grounding_source</span><span class="o">.</span><span class="n">_to_grounding_source_dict</span><span class="p">()</span>

        <span class="n">message_structs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">past_message</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="p">:</span>
            <span class="n">message_structs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">past_message</span><span class="o">.</span><span class="n">author</span><span class="p">,</span>
                    <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">past_message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="n">message_structs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">message</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

        <span class="n">prediction_instance</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">message_structs</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context</span><span class="p">:</span>
            <span class="n">prediction_instance</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_examples</span><span class="p">:</span>
            <span class="n">prediction_instance</span><span class="p">[</span><span class="s2">&quot;examples&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">example</span><span class="o">.</span><span class="n">input_text</span><span class="p">},</span>
                    <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">example</span><span class="o">.</span><span class="n">output_text</span><span class="p">},</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_examples</span>
            <span class="p">]</span>

        <span class="k">return</span> <span class="n">_PredictionRequest</span><span class="p">(</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_parameters</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_parse_chat_prediction_response</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">prediction_response</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span>
        <span class="n">prediction_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiCandidateTextGenerationResponse</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Parses prediction response for chat models.</span>

<span class="sd">        Args:</span>
<span class="sd">            prediction_response: Prediction response received from the model</span>
<span class="sd">            prediction_idx: Index of the prediction to parse.</span>
<span class="sd">            candidate_idx: Index of the candidate to parse.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">[</span><span class="n">prediction_idx</span><span class="p">]</span>
        <span class="n">candidate_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;candidates&quot;</span><span class="p">])</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">grounding_metadata_list</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;groundingMetadata&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">candidate_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">candidate_count</span><span class="p">):</span>
            <span class="n">safety_attributes</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;safetyAttributes&quot;</span><span class="p">][</span><span class="n">candidate_idx</span><span class="p">]</span>
            <span class="n">errors_list</span> <span class="o">=</span> <span class="n">safety_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;errors&quot;</span><span class="p">,</span> <span class="p">[])</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">errors_list</span><span class="p">))</span>
            <span class="n">grounding_metadata_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">if</span> <span class="n">grounding_metadata_list</span> <span class="ow">and</span> <span class="n">grounding_metadata_list</span><span class="p">[</span><span class="n">candidate_idx</span><span class="p">]:</span>
                <span class="n">grounding_metadata_dict</span> <span class="o">=</span> <span class="n">grounding_metadata_list</span><span class="p">[</span><span class="n">candidate_idx</span><span class="p">]</span>
            <span class="n">candidate_response</span> <span class="o">=</span> <span class="n">TextGenerationResponse</span><span class="p">(</span>
                <span class="n">text</span><span class="o">=</span><span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;candidates&quot;</span><span class="p">][</span><span class="n">candidate_idx</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">],</span>
                <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
                <span class="n">is_blocked</span><span class="o">=</span><span class="n">safety_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;blocked&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
                <span class="n">errors</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span>
                <span class="n">safety_attributes</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
                    <span class="nb">zip</span><span class="p">(</span>
                        <span class="c1"># Unlike with normal prediction, in streaming prediction</span>
                        <span class="c1"># categories and scores can be None</span>
                        <span class="n">safety_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;categories&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">[],</span>
                        <span class="n">safety_attributes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;scores&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">[],</span>
                    <span class="p">)</span>
                <span class="p">),</span>
                <span class="n">grounding_metadata</span><span class="o">=</span><span class="n">GroundingMetadata</span><span class="p">(</span><span class="n">grounding_metadata_dict</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">candidates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">candidate_response</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">MultiCandidateTextGenerationResponse</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">,</span>
            <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
            <span class="n">is_blocked</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_blocked</span><span class="p">,</span>
            <span class="n">errors</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">errors</span><span class="p">,</span>
            <span class="n">safety_attributes</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">safety_attributes</span><span class="p">,</span>
            <span class="n">grounding_metadata</span><span class="o">=</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grounding_metadata</span><span class="p">,</span>
            <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>
<span class="sd">            grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
            <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">response_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
        <span class="p">)</span>
        <span class="n">response_text</span> <span class="o">=</span> <span class="n">response_obj</span><span class="o">.</span><span class="n">text</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">response_obj</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
                <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>
<span class="sd">            grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains</span>
<span class="sd">            the text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
            <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">response_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
        <span class="p">)</span>
        <span class="n">response_text</span> <span class="o">=</span> <span class="n">response_obj</span><span class="o">.</span><span class="n">text</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">response_obj</span>

    <span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_service_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span>

        <span class="n">full_response_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="k">for</span> <span class="p">(</span>
            <span class="n">prediction_dict</span>
        <span class="p">)</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict</span><span class="p">(</span>
            <span class="n">prediction_service_client</span><span class="o">=</span><span class="n">prediction_service_client</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">prediction_response</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
                <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
                <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">text_generation_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
                <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
            <span class="p">)</span>
            <span class="n">full_response_text</span> <span class="o">+=</span> <span class="n">text_generation_response</span><span class="o">.</span><span class="n">text</span>
            <span class="k">yield</span> <span class="n">text_generation_response</span>

        <span class="c1"># We only add the question and answer to the history if/when the answer</span>
        <span class="c1"># was read fully. Otherwise, the answer would have been truncated.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">full_response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_service_async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_async_client</span>

        <span class="n">full_response_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="k">async</span> <span class="k">for</span> <span class="n">prediction_dict</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict_async</span><span class="p">(</span>
            <span class="n">prediction_service_async_client</span><span class="o">=</span><span class="n">prediction_service_async_client</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">prediction_response</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
                <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
                <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">text_generation_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
                <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
            <span class="p">)</span>
            <span class="n">full_response_text</span> <span class="o">+=</span> <span class="n">text_generation_response</span><span class="o">.</span><span class="n">text</span>
            <span class="k">yield</span> <span class="n">text_generation_response</span>

        <span class="c1"># We only add the question and answer to the history if/when the answer</span>
        <span class="c1"># was read fully. Otherwise, the answer would have been truncated.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">full_response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_ChatSessionBaseWithCountTokensMixin</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A mixin class for adding count_tokens to ChatSession.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CountTokensResponse</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Counts the tokens and billable characters for the provided chat message and any message history,</span>
<span class="sd">        context, or examples set on the chat session.</span>

<span class="sd">        If you&#39;ve called `send_message()` in the current chat session before calling `count_tokens()`, the</span>
<span class="sd">        response will include the total tokens and characters for the previously sent message and the one in the</span>
<span class="sd">        `count_tokens()` request. To count the tokens for a single message, call `count_tokens()` right after</span>
<span class="sd">        calling `start_chat()` before calling `send_message()`.</span>

<span class="sd">        Note: this does not make a prediction request to the model, it only counts the tokens</span>
<span class="sd">        in the request.</span>

<span class="sd">        Examples::</span>

<span class="sd">        model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)</span>
<span class="sd">        chat_session = model.start_chat()</span>
<span class="sd">        count_tokens_response = chat_session.count_tokens(&quot;How&#39;s it going?&quot;)</span>

<span class="sd">        count_tokens_response.total_tokens</span>
<span class="sd">        count_tokens_response.total_billable_characters</span>

<span class="sd">        Args:</span>
<span class="sd">            message (str):</span>
<span class="sd">                Required. A chat message to count tokens or. For example: &quot;How&#39;s it going?&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            A `CountTokensResponse` object that contains the number of tokens</span>
<span class="sd">            in the text and the number of billable characters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">count_tokens_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span><span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">)</span>

        <span class="n">count_tokens_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span><span class="o">.</span><span class="n">select_version</span><span class="p">(</span>
            <span class="s2">&quot;v1beta1&quot;</span>
        <span class="p">)</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span>
            <span class="n">endpoint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">count_tokens_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">CountTokensResponse</span><span class="p">(</span>
            <span class="n">total_tokens</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="o">.</span><span class="n">total_tokens</span><span class="p">,</span>
            <span class="n">total_billable_characters</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="o">.</span><span class="n">total_billable_characters</span><span class="p">,</span>
            <span class="n">_count_tokens_response</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">_PreviewChatSession</span><span class="p">(</span><span class="n">_ChatSessionBaseWithCountTokensMixin</span><span class="p">):</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>


<span class="k">class</span> <span class="nc">_PreviewCodeChatSession</span><span class="p">(</span><span class="n">_ChatSessionBaseWithCountTokensMixin</span><span class="p">):</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>


<span class="k">class</span> <span class="nc">ChatSession</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatSession represents a chat session with a language model.</span>

<span class="sd">    Within a chat session, the model keeps context and remembers the previous conversation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ChatModel</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">CodeChatSession</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CodeChatSession represents a chat session with code chat language model.</span>

<span class="sd">    Within a code chat session, the model keeps context and remembers the previous converstion.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">CodeChatModel</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="CodeChatSession.send_message"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models.CodeChatSession.send_message">[docs]</a>    <span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the code chat model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">                Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">                 Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="CodeChatSession.send_message_async"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models.CodeChatSession.send_message_async">[docs]</a>    <span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the code chat model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">                Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">                 Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span></div>

<div class="viewcode-block" id="CodeChatSession.send_message_streaming"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models.CodeChatSession.send_message_streaming">[docs]</a>    <span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="CodeChatSession.send_message_streaming_async"><a class="viewcode-back" href="../../../vertexai/services.html#vertexai.language_models.CodeChatSession.send_message_streaming_async">[docs]</a>    <span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming_async</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_CodeGenerationModel</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A language model that generates code.</span>

<span class="sd">    Examples:</span>

<span class="sd">        # Getting answers:</span>
<span class="sd">        generation_model = CodeGenerationModel.from_pretrained(&quot;code-bison@001&quot;)</span>
<span class="sd">        print(generation_model.predict(</span>
<span class="sd">            prefix=&quot;Write a function that checks if a year is a leap year.&quot;,</span>
<span class="sd">        ))</span>

<span class="sd">        completion_model = CodeGenerationModel.from_pretrained(&quot;code-gecko@001&quot;)</span>
<span class="sd">        print(completion_model.predict(</span>
<span class="sd">            prefix=&quot;def reverse_string(s):&quot;,</span>
<span class="sd">        ))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/code_generation_1.0.0.yaml&quot;</span>

    <span class="k">def</span> <span class="nf">_create_prediction_request</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_PredictionRequest</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a code generation prediction request.</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix: Code before the current point.</span>
<span class="sd">            suffix: Code after the current point.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of response candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `TextGenerationResponse` object that contains the text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prefix&quot;</span><span class="p">:</span> <span class="n">prefix</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">suffix</span><span class="p">:</span>
            <span class="n">instance</span><span class="p">[</span><span class="s2">&quot;suffix&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">suffix</span>

        <span class="n">prediction_parameters</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;temperature&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">temperature</span>

        <span class="k">if</span> <span class="n">max_output_tokens</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;maxOutputTokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_output_tokens</span>

        <span class="k">if</span> <span class="n">stop_sequences</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;stopSequences&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">stop_sequences</span>

        <span class="k">if</span> <span class="n">candidate_count</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prediction_parameters</span><span class="p">[</span><span class="s2">&quot;candidateCount&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate_count</span>

        <span class="k">return</span> <span class="n">_PredictionRequest</span><span class="p">(</span><span class="n">instance</span><span class="o">=</span><span class="n">instance</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_parameters</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets model response for a single prompt.</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix: Code before the current point.</span>
<span class="sd">            suffix: Code after the current point.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of response candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_prediction_request</span><span class="p">(</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">suffix</span><span class="o">=</span><span class="n">suffix</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
            <span class="n">prediction_response</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously gets model response for a single prompt.</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix: Code before the current point.</span>
<span class="sd">            suffix: Code after the current point.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of response candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_prediction_request</span><span class="p">(</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">suffix</span><span class="o">=</span><span class="n">suffix</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
            <span class="n">prediction_response</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Predicts the code based on previous code.</span>

<span class="sd">        The result is a stream (generator) of partial responses.</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix: Code before the current point.</span>
<span class="sd">            suffix: Code after the current point.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_prediction_request</span><span class="p">(</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">suffix</span><span class="o">=</span><span class="n">suffix</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_service_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span>
        <span class="k">for</span> <span class="p">(</span>
            <span class="n">prediction_dict</span>
        <span class="p">)</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict</span><span class="p">(</span>
            <span class="n">prediction_service_client</span><span class="o">=</span><span class="n">prediction_service_client</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">prediction_obj</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
                <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
                <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">yield</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span><span class="n">prediction_obj</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously predicts the code based on previous code.</span>

<span class="sd">        The result is a stream (generator) of partial responses.</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix: Code before the current point.</span>
<span class="sd">            suffix: Code after the current point.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_prediction_request</span><span class="p">(</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">suffix</span><span class="o">=</span><span class="n">suffix</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prediction_service_async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_async_client</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">prediction_dict</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict_async</span><span class="p">(</span>
            <span class="n">prediction_service_async_client</span><span class="o">=</span><span class="n">prediction_service_async_client</span><span class="p">,</span>
            <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
            <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">prediction_obj</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
                <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
                <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">yield</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span><span class="n">prediction_obj</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_CountTokensCodeGenerationMixin</span><span class="p">(</span><span class="n">_LanguageModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mixin for code generation models that support the CountTokens API&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CountTokensResponse</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Counts the tokens and billable characters for a given code generation prompt.</span>

<span class="sd">        Note: this does not make a prediction request to the model, it only counts the tokens</span>
<span class="sd">        in the request.</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix (str): Code before the current point.</span>
<span class="sd">            suffix (str): Code after the current point.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `CountTokensResponse` object that contains the number of tokens</span>
<span class="sd">            in the text and the number of billable characters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction_request</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prefix&quot;</span><span class="p">:</span> <span class="n">prefix</span><span class="p">,</span> <span class="s2">&quot;suffix&quot;</span><span class="p">:</span> <span class="n">suffix</span><span class="p">}</span>

        <span class="n">count_tokens_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span><span class="o">.</span><span class="n">select_version</span><span class="p">(</span>
            <span class="s2">&quot;v1beta1&quot;</span>
        <span class="p">)</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span>
            <span class="n">endpoint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
            <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">CountTokensResponse</span><span class="p">(</span>
            <span class="n">total_tokens</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="o">.</span><span class="n">total_tokens</span><span class="p">,</span>
            <span class="n">total_billable_characters</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="o">.</span><span class="n">total_billable_characters</span><span class="p">,</span>
            <span class="n">_count_tokens_response</span><span class="o">=</span><span class="n">count_tokens_response</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">class</span> <span class="nc">CodeGenerationModel</span><span class="p">(</span>
    <span class="n">_CodeGenerationModel</span><span class="p">,</span>
    <span class="n">_TunableTextModelMixin</span><span class="p">,</span>
    <span class="n">_ModelWithBatchPredict</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">pass</span>


<span class="k">class</span> <span class="nc">_PreviewCodeGenerationModel</span><span class="p">(</span><span class="n">CodeGenerationModel</span><span class="p">,</span> <span class="n">_CountTokensCodeGenerationMixin</span><span class="p">):</span>
    <span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;CodeGenerationModel&quot;</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>


<span class="c1">###### Model tuning</span>
<span class="c1"># Currently, tuning can only work in this location</span>

<span class="n">_SUPPORTED_LOCATIONS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># 1</span>
    <span class="s2">&quot;us-central1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;europe-west4&quot;</span><span class="p">,</span>
    <span class="s2">&quot;asia-southeast1&quot;</span><span class="p">,</span>
    <span class="c1"># 2</span>
    <span class="s2">&quot;us-west1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;europe-west3&quot;</span><span class="p">,</span>
    <span class="s2">&quot;europe-west2&quot;</span><span class="p">,</span>
    <span class="s2">&quot;asia-northeast1&quot;</span><span class="p">,</span>
    <span class="c1"># 3</span>
    <span class="s2">&quot;us-east4&quot;</span><span class="p">,</span>
    <span class="s2">&quot;us-west4&quot;</span><span class="p">,</span>
    <span class="s2">&quot;northamerica-northeast1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;europe-west9&quot;</span><span class="p">,</span>
    <span class="s2">&quot;europe-west1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;asia-northeast3&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">_TUNING_LOCATIONS</span> <span class="o">=</span> <span class="n">_SUPPORTED_LOCATIONS</span>
<span class="c1"># Currently, deployment can only work in these locations</span>
<span class="n">_TUNED_MODEL_LOCATIONS</span> <span class="o">=</span> <span class="n">_SUPPORTED_LOCATIONS</span>

<span class="c1"># All models supported by RLHF that can also be used for online and batch prediction:</span>
<span class="n">_SUPPORTED_RLHF_MODELS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;text-bison@001&quot;</span><span class="p">,</span>
    <span class="s2">&quot;chat-bison@001&quot;</span><span class="p">,</span>
    <span class="s2">&quot;text-bison@002&quot;</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span> <span class="nf">_get_invalid_tuning_location_msg</span><span class="p">(</span>
    <span class="n">requested_location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">valid_locations</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constructs error message when user requests an invalid tuning location.</span>

<span class="sd">    Args:</span>
<span class="sd">        requested_location: Invalid location requested by a user.</span>
<span class="sd">        valid_locations: All locations supported by the tuning method.</span>

<span class="sd">    Returns:</span>
<span class="sd">        User-facing error message.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Tuning is not supported in `</span><span class="si">{</span><span class="n">requested_location</span><span class="si">}</span><span class="s2">`.&quot;</span>
        <span class="s2">&quot;Please specify a valid tuning_job_location from the following &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;supported regions: </span><span class="si">{</span><span class="n">valid_locations</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_invalid_rlhf_model_msg</span><span class="p">(</span>
    <span class="n">requested_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constructs error message when user tries to tune an unsupported model.</span>

<span class="sd">    Args:</span>
<span class="sd">        requested_model: Invalid model requested by a user.</span>
<span class="sd">        valid_models: All models supported by the tuning method.</span>

<span class="sd">    Returns:</span>
<span class="sd">        User-facing error message.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Model </span><span class="si">{</span><span class="n">requested_model</span><span class="si">}</span><span class="s2"> does not support tuning with RLHF. &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Please use one of the supported models: </span><span class="si">{</span><span class="n">_SUPPORTED_RLHF_MODELS</span><span class="si">}</span><span class="s2">.&quot;</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">_TuningJob</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TuningJob represents a fine-tuning job.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">job</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">PipelineJob</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Internal constructor. Do not call directly.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_job</span> <span class="o">=</span> <span class="n">job</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tuned_model_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_get_tuned_model_name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extracts the tuned model name from the tuning pipeline job.</span>

<span class="sd">        This method is used for both tuning, RLHF and distillation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The Vertex Model resource name of the tuned model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tuned_model_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tuned_model_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_job</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

        <span class="c1"># Getting tuned model from the pipeline.</span>
        <span class="n">model_task</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Searching for the model uploading task first.</span>
        <span class="c1"># Note: Distillation does not have pipeline outputs yet.</span>
        <span class="n">upload_model_task_names</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;upload-llm-model&quot;</span><span class="p">,</span>  <span class="c1"># Most tuning pipelines</span>
            <span class="s2">&quot;upload-model&quot;</span><span class="p">,</span>  <span class="c1"># New distillation pipeline uses &quot;upload-model&quot;</span>
            <span class="s2">&quot;text-embedding-model-uploader&quot;</span><span class="p">,</span>  <span class="c1"># Embedding model tuning pipelines.</span>
        <span class="p">]</span>
        <span class="n">upload_model_tasks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">task_info</span>
            <span class="k">for</span> <span class="n">task_info</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_job</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">job_detail</span><span class="o">.</span><span class="n">task_details</span>
            <span class="k">if</span> <span class="n">task_info</span><span class="o">.</span><span class="n">task_name</span> <span class="ow">in</span> <span class="n">upload_model_task_names</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">upload_model_tasks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">model_task</span> <span class="o">=</span> <span class="n">upload_model_tasks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_task</span><span class="p">:</span>
            <span class="n">root_pipeline_tasks</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">task_detail</span>
                <span class="k">for</span> <span class="n">task_detail</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_job</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">job_detail</span><span class="o">.</span><span class="n">task_details</span>
                <span class="k">if</span> <span class="n">task_detail</span><span class="o">.</span><span class="n">execution</span><span class="o">.</span><span class="n">schema_title</span> <span class="o">==</span> <span class="s2">&quot;system.Run&quot;</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">root_pipeline_tasks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">model_task</span> <span class="o">=</span> <span class="n">root_pipeline_tasks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_task</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Failed to get the model name from the tuning pipeline: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_job</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Trying to get model name from output parameter</span>
        <span class="n">vertex_model_name</span> <span class="o">=</span> <span class="n">model_task</span><span class="o">.</span><span class="n">execution</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span>
            <span class="s2">&quot;output:model_resource_name&quot;</span>
        <span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tuning has completed. Created Vertex Model: </span><span class="si">{</span><span class="n">vertex_model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tuned_model_name</span> <span class="o">=</span> <span class="n">vertex_model_name</span>
        <span class="k">return</span> <span class="n">vertex_model_name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_status</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">aiplatform_types</span><span class="o">.</span><span class="n">pipeline_state</span><span class="o">.</span><span class="n">PipelineState</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Job status.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_job</span><span class="o">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">_cancel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cancels the job.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_job</span><span class="o">.</span><span class="n">cancel</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_LanguageModelTuningJob</span><span class="p">(</span><span class="n">_TuningJob</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LanguageModelTuningJob represents a fine-tuning job.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">base_model</span><span class="p">:</span> <span class="n">_LanguageModel</span><span class="p">,</span>
        <span class="n">job</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">PipelineJob</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Internal constructor. Do not call directly.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">job</span><span class="o">=</span><span class="n">job</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_model</span> <span class="o">=</span> <span class="n">base_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_LanguageModel</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Blocks until the tuning is complete and returns a `LanguageModel` object.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>
        <span class="n">vertex_model_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tuned_model_name</span><span class="p">()</span>
        <span class="n">_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tuning has completed. Created Vertex Model: </span><span class="si">{</span><span class="n">vertex_model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_base_model</span><span class="p">)</span><span class="o">.</span><span class="n">get_tuned_model</span><span class="p">(</span>
            <span class="n">tuned_model_name</span><span class="o">=</span><span class="n">vertex_model_name</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>


<span class="k">class</span> <span class="nc">_TextEmbeddingModelTuningJob</span><span class="p">(</span><span class="n">_LanguageModelTuningJob</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;_TextEmbeddingModelTuningJob represents a tuning job.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Use deploy_tuned_model instead to get the tuned model.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">deploy_tuned_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">machine_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">accelerator_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextEmbeddingModel&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Gets the tuned model.</span>

<span class="sd">        Args:</span>
<span class="sd">            machine_type: Machine type. E.g., &quot;a2-highgpu-1g&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator: Kind of accelerator. E.g., &quot;NVIDIA_TESLA_A100&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">            accelerator_count: Count of accelerators.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuned `LanguageModel` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>
        <span class="n">vertex_model_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tuned_model_name</span><span class="p">()</span>
        <span class="n">_LOGGER</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tuning has completed. Created Vertex Model: </span><span class="si">{</span><span class="n">vertex_model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">TextEmbeddingModel</span><span class="o">.</span><span class="n">deploy_tuned_model</span><span class="p">(</span>
            <span class="n">tuned_model_name</span><span class="o">=</span><span class="n">vertex_model_name</span><span class="p">,</span>
            <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
            <span class="n">accelerator</span><span class="o">=</span><span class="n">accelerator</span><span class="p">,</span>
            <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>


<span class="k">def</span> <span class="nf">_get_tuned_models_dir_uri</span><span class="p">(</span><span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">staging_bucket</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">staging_bucket</span>
            <span class="o">+</span> <span class="s2">&quot;/tuned_language_models/&quot;</span>
            <span class="o">+</span> <span class="n">model_id</span>
        <span class="p">)</span>
    <span class="n">staging_gcs_bucket</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">gcs_utils</span><span class="o">.</span><span class="n">create_gcs_bucket_for_pipeline_artifacts_if_it_does_not_exist</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">staging_gcs_bucket</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/output_artifacts/&quot;</span><span class="p">,</span> <span class="s2">&quot;/tuned_language_models/&quot;</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">model_id</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_list_tuned_model_names</span><span class="p">(</span><span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">tuned_models</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">list</span><span class="p">(</span>
        <span class="nb">filter</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;labels.</span><span class="si">{</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="si">}</span><span class="s1">=&quot;</span><span class="si">{</span><span class="n">model_id</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;-&quot;</span><span class="p">)</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">,</span>
        <span class="c1"># TODO(b/275444096): Remove the explicit location once models are deployed to the user&#39;s selected location</span>
        <span class="n">location</span><span class="o">=</span><span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">resource_name</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">tuned_models</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model_names</span>


<span class="k">def</span> <span class="nf">_generate_tuned_model_dir_uri</span><span class="p">(</span><span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">tuned_model_id</span> <span class="o">=</span> <span class="s2">&quot;tuned_model_&quot;</span> <span class="o">+</span> <span class="n">aiplatform_utils</span><span class="o">.</span><span class="n">timestamped_unique_name</span><span class="p">()</span>
    <span class="n">tuned_models_dir_uri</span> <span class="o">=</span> <span class="n">_get_tuned_models_dir_uri</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">)</span>
    <span class="n">tuned_model_dir_uri</span> <span class="o">=</span> <span class="n">_uri_join</span><span class="p">(</span><span class="n">tuned_models_dir_uri</span><span class="p">,</span> <span class="n">tuned_model_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tuned_model_dir_uri</span>


<span class="k">def</span> <span class="nf">_maybe_upload_training_data</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Uploads training DataFrame to GCS (if necessary) and returns its location.</span>

<span class="sd">    Args:</span>
<span class="sd">        training_data: Dataframe to upload, or GCS path if already uploaded.</span>
<span class="sd">        model_id: Model ID used to generate upload URI.</span>

<span class="sd">    Returns:</span>
<span class="sd">        GCS dataset URI or name if dataset name was provided.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If training data is not a string or DataFrame.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">training_data</span>
    <span class="k">elif</span> <span class="n">pandas</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
        <span class="n">output_dir_uri</span> <span class="o">=</span> <span class="n">_generate_tuned_model_dir_uri</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">)</span>
        <span class="n">dataset_uri</span> <span class="o">=</span> <span class="n">_uri_join</span><span class="p">(</span><span class="n">output_dir_uri</span><span class="p">,</span> <span class="s2">&quot;training_data.jsonl&quot;</span><span class="p">)</span>

        <span class="n">gcs_utils</span><span class="o">.</span><span class="n">_upload_pandas_df_to_gcs</span><span class="p">(</span>
            <span class="n">df</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span> <span class="n">upload_gcs_path</span><span class="o">=</span><span class="n">dataset_uri</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">dataset_uri</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Unsupported training_data type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
            <span class="s2">&quot;Must be a string or pandas DataFrame.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_launch_tuning_job</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">tuning_pipeline_uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">tuning_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">PipelineJob</span><span class="p">:</span>
    <span class="n">training_data_path</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">model_display_name</span><span class="p">:</span>
        <span class="c1"># Creating a human-readable model display name</span>
        <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_id</span><span class="si">}</span><span class="s2"> tuned&quot;</span>

        <span class="n">train_steps</span> <span class="o">=</span> <span class="n">tuning_parameters</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train_steps&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train_steps</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; for </span><span class="si">{</span><span class="n">train_steps</span><span class="si">}</span><span class="s2"> steps&quot;</span>

        <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">tuning_parameters</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">learning_rate</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; with learning rate </span><span class="si">{</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">learning_rate_multiplier</span> <span class="o">=</span> <span class="n">tuning_parameters</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;learning_rate_multiplier&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">learning_rate_multiplier</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; with learning_rate_multiplier=</span><span class="si">{</span><span class="n">learning_rate_multiplier</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="n">name</span> <span class="o">+=</span> <span class="s2">&quot; on &quot;</span>
        <span class="c1"># Truncating the start of the dataset URI to keep total length &lt;= 128.</span>
        <span class="n">max_display_name_length</span> <span class="o">=</span> <span class="mi">128</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_data_path</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">max_display_name_length</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">+=</span> <span class="n">training_data_path</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">+=</span> <span class="s2">&quot;...&quot;</span>
            <span class="n">remaining_length</span> <span class="o">=</span> <span class="n">max_display_name_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">name</span> <span class="o">+=</span> <span class="n">training_data_path</span><span class="p">[</span><span class="o">-</span><span class="n">remaining_length</span><span class="p">:]</span>
        <span class="n">model_display_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">[:</span><span class="n">max_display_name_length</span><span class="p">]</span>

    <span class="n">pipeline_arguments</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;project&quot;</span><span class="p">:</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">project</span><span class="p">,</span>
        <span class="c1"># TODO(b/275444096): Remove the explicit location once tuning can happen in all regions</span>
        <span class="c1"># &quot;location&quot;: aiplatform_initializer.global_config.location,</span>
        <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="n">tuned_model_location</span><span class="p">,</span>
        <span class="s2">&quot;model_display_name&quot;</span><span class="p">:</span> <span class="n">model_display_name</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">tuning_pipeline_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span>
        <span class="s2">&quot;https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model&quot;</span>
    <span class="p">):</span>
        <span class="n">pipeline_arguments</span><span class="p">[</span><span class="s2">&quot;train_label_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">training_data_path</span>
    <span class="k">elif</span> <span class="n">training_data_path</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
        <span class="n">pipeline_arguments</span><span class="p">[</span><span class="s2">&quot;dataset_uri&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">training_data_path</span>
    <span class="k">elif</span> <span class="n">training_data_path</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;projects/&quot;</span><span class="p">):</span>
        <span class="n">pipeline_arguments</span><span class="p">[</span><span class="s2">&quot;dataset_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">training_data_path</span>
    <span class="n">pipeline_arguments</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tuning_parameters</span><span class="p">)</span>
    <span class="n">job</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">PipelineJob</span><span class="p">(</span>
        <span class="n">template_path</span><span class="o">=</span><span class="n">tuning_pipeline_uri</span><span class="p">,</span>
        <span class="n">display_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">parameter_values</span><span class="o">=</span><span class="n">pipeline_arguments</span><span class="p">,</span>
        <span class="c1"># TODO(b/275444101): Remove the explicit location once model can be deployed in all regions</span>
        <span class="n">location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">job</span><span class="o">.</span><span class="n">submit</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">job</span>


<span class="k">def</span> <span class="nf">_launch_rlhf_tuning_job</span><span class="p">(</span>
    <span class="n">tuning_parameters</span><span class="p">:</span> <span class="n">_RlhfTuningParameters</span><span class="p">,</span>
    <span class="n">tuning_pipeline_uri</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">_DEFAULT_RLHF_TUNING_PIPELINE_URI</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">PipelineJob</span><span class="p">:</span>
    <span class="n">job</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">PipelineJob</span><span class="p">(</span>
        <span class="n">template_path</span><span class="o">=</span><span class="n">tuning_pipeline_uri</span><span class="p">,</span>
        <span class="n">display_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">parameter_values</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="o">.</span><span class="n">asdict</span><span class="p">(),</span>
        <span class="c1"># TODO(b/275444101): Remove the explicit location once model can be deployed in all regions</span>
        <span class="n">location</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="o">.</span><span class="n">location</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">job</span><span class="o">.</span><span class="n">submit</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">job</span>


<span class="k">def</span> <span class="nf">_uri_join</span><span class="p">(</span><span class="n">uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">path_fragment</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Appends path fragment to URI.</span>

<span class="sd">    urllib.parse.urljoin only works on URLs, not URIs</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">uri</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">path_fragment</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>


<span class="c1"># Importing here to prevent issues caused by circular references</span>
<span class="c1"># pylint: disable=g-import-not-at-top,g-bad-import-order</span>
<span class="kn">from</span> <span class="nn">vertexai.language_models</span> <span class="kn">import</span> <span class="n">_distillation</span>


<span class="k">class</span> <span class="nc">_PreviewTextGenerationModel</span><span class="p">(</span>
    <span class="n">_TextGenerationModel</span><span class="p">,</span>
    <span class="n">_PreviewTunableTextModelMixin</span><span class="p">,</span>
    <span class="n">_PreviewModelWithBatchPredict</span><span class="p">,</span>
    <span class="n">_evaluatable_language_models</span><span class="o">.</span><span class="n">_EvaluatableLanguageModel</span><span class="p">,</span>
    <span class="n">_CountTokensMixin</span><span class="p">,</span>
    <span class="n">_distillation</span><span class="o">.</span><span class="n">DistillationMixin</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Do not add docstring so that it&#39;s inherited from the base class.</span>
    <span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;TextGenerationModel&quot;</span>
    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.preview.language_models&quot;</span>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">google-cloud-aiplatform</a></h1>



<p class="blurb">Google Cloud Client Libraries for google-cloud-aiplatform</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=googleapis&repo=python-aiplatform&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../vertexai/services.html">Vertex AI SDK</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  <li><a href="../language_models.html">vertexai.language_models</a><ul>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2019, Google.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
    </div>

    
    <a href="https://github.com/googleapis/python-aiplatform" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>